arguments: .\src\train_tripletloss.py
--------------------
tensorflow version: 2.3.1
--------------------
git hash: b'096ed770f163957c1e56efa7feeb194773920f6e'
--------------------
b'diff --git a/contributed/batch_represent.py b/contributed/batch_represent.py\nindex 9ec4481..6896476 100755\n--- a/contributed/batch_represent.py\n+++ b/contributed/batch_represent.py\n@@ -82,7 +82,7 @@ def main(args):\n \n \twith tf.Graph().as_default():\n \n-\t\twith tf.Session() as sess:\n+\t\twith tf.compat.v1.Session() as sess:\n \n \t\t\t# create output directory if it doesn\'t exist\n \t\t\toutput_dir = os.path.expanduser(args.output_dir)\n@@ -101,9 +101,9 @@ def main(args):\n \t\t\tpaths = data[\'filenames\']\n \n \t\t\t# Get input and output tensors\n-\t\t\timages_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-\t\t\tembeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-\t\t\tphase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n+\t\t\timages_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n+\t\t\tembeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n+\t\t\tphase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n \n \t\t\timage_size = images_placeholder.get_shape()[1]\n \t\t\tembedding_size = embeddings.get_shape()[1]\ndiff --git a/contributed/cluster.py b/contributed/cluster.py\nindex 6bd1899..6319bfa 100644\n--- a/contributed/cluster.py\n+++ b/contributed/cluster.py\n@@ -42,7 +42,7 @@ def main(args):\n \n     with tf.Graph().as_default():\n \n-        with tf.Session() as sess:\n+        with tf.compat.v1.Session() as sess:\n             facenet.load_model(args.model)\n \n             image_list = load_images_from_folder(args.data_dir)\n@@ -148,8 +148,8 @@ def align_data(image_list, image_size, margin, pnet, rnet, onet):\n \n def create_network_face_detection(gpu_memory_fraction):\n     with tf.Graph().as_default():\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n+        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n         with sess.as_default():\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n     return pnet, rnet, onet\ndiff --git a/contributed/clustering.py b/contributed/clustering.py\nindex 9e53c23..24c137f 100644\n--- a/contributed/clustering.py\n+++ b/contributed/clustering.py\n@@ -23,8 +23,8 @@ def face_distance(face_encodings, face_to_compare):\n \n def load_model(model_dir, meta_file, ckpt_file):\n     model_dir_exp = os.path.expanduser(model_dir)\n-    saver = tf.train.import_meta_graph(os.path.join(model_dir_exp, meta_file))\n-    saver.restore(tf.get_default_session(), os.path.join(model_dir_exp, ckpt_file))\n+    saver = tf.compat.v1.train.import_meta_graph(os.path.join(model_dir_exp, meta_file))\n+    saver.restore(tf.compat.v1.get_default_session(), os.path.join(model_dir_exp, ckpt_file))\n \n def _chinese_whispers(encoding_list, threshold=0.55, iterations=20):\n     """ Chinese Whispers Algorithm\n@@ -211,7 +211,7 @@ def main(args):\n         makedirs(args.output)\n \n     with tf.Graph().as_default():\n-        with tf.Session() as sess:\n+        with tf.compat.v1.Session() as sess:\n             image_paths = get_onedir(args.input)\n             #image_list, label_list = facenet.get_image_paths_and_labels(train_set)\n \n@@ -222,9 +222,9 @@ def main(args):\n             load_model(args.model_dir, meta_file, ckpt_file)\n             \n             # Get input and output tensors\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n             \n             image_size = images_placeholder.get_shape()[1]\n             print("image_size:",image_size)\ndiff --git a/contributed/export_embeddings.py b/contributed/export_embeddings.py\nindex d378c2d..7f72266 100644\n--- a/contributed/export_embeddings.py\n+++ b/contributed/export_embeddings.py\n@@ -77,15 +77,15 @@ def main(args):\n \n     with tf.Graph().as_default():\n \n-        with tf.Session() as sess:\n+        with tf.compat.v1.Session() as sess:\n \n             # Load the model\n             facenet.load_model(args.model_dir)\n \n             # Get input and output tensors\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n \n             # Run forward pass to calculate embeddings\n             nrof_images = len(image_list)\n@@ -136,8 +136,8 @@ def load_and_align_data(image_paths, image_size, margin, gpu_memory_fraction):\n \n     print(\'Creating networks and loading parameters\')\n     with tf.Graph().as_default():\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n+        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n         with sess.as_default():\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n \ndiff --git a/contributed/face.py b/contributed/face.py\nindex 97b9500..642eb18 100644\n--- a/contributed/face.py\n+++ b/contributed/face.py\n@@ -97,15 +97,15 @@ class Identifier:\n \n class Encoder:\n     def __init__(self):\n-        self.sess = tf.Session()\n+        self.sess = tf.compat.v1.Session()\n         with self.sess.as_default():\n             facenet.load_model(facenet_model_checkpoint)\n \n     def generate_embedding(self, face):\n         # Get input and output tensors\n-        images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-        embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-        phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n+        images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n+        embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n+        phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n \n         prewhiten_face = facenet.prewhiten(face.image)\n \n@@ -127,8 +127,8 @@ class Detection:\n \n     def _setup_mtcnn(self):\n         with tf.Graph().as_default():\n-            gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-            sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+            gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n+            sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n             with sess.as_default():\n                 return align.detect_face.create_mtcnn(sess, None)\n \ndiff --git a/contributed/predict.py b/contributed/predict.py\nindex 8bb10a8..8ad6006 100644\n--- a/contributed/predict.py\n+++ b/contributed/predict.py\n@@ -47,14 +47,14 @@ def main(args):\n     images, cout_per_image, nrof_samples = load_and_align_data(args.image_files,args.image_size, args.margin, args.gpu_memory_fraction)\n     with tf.Graph().as_default():\n \n-       with tf.Session() as sess:\n+       with tf.compat.v1.Session() as sess:\n       \n             # Load the model\n                 facenet.load_model(args.model)\n             # Get input and output tensors\n-                images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-                embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-                phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n+                images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n+                embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n+                phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n \n             # Run forward pass to calculate embeddings\n                 feed_dict = { images_placeholder: images , phase_train_placeholder:False}\n@@ -82,8 +82,8 @@ def load_and_align_data(image_paths, image_size, margin, gpu_memory_fraction):\n     \n     print(\'Creating networks and loading parameters\')\n     with tf.Graph().as_default():\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n+        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n         with sess.as_default():\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n   \ndiff --git a/requirements.txt b/requirements.txt\nindex b7418c9..7d762bf 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,4 +1,3 @@\n-tensorflow==1.7\n scipy\n scikit-learn\n opencv-python\n@@ -7,3 +6,4 @@ matplotlib\n Pillow\n requests\n psutil\n+tf_slim\n\\ No newline at end of file\ndiff --git a/src/align/align_dataset_mtcnn.py b/src/align/align_dataset_mtcnn.py\nindex 7d5e735..fe6cf62 100644\n--- a/src/align/align_dataset_mtcnn.py\n+++ b/src/align/align_dataset_mtcnn.py\n@@ -49,8 +49,8 @@ def main(args):\n     print(\'Creating networks and loading parameters\')\n     \n     with tf.Graph().as_default():\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n+        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n         with sess.as_default():\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n     \ndiff --git a/src/align/detect_face.py b/src/align/detect_face.py\nindex 7f98ca7..907de39 100644\n--- a/src/align/detect_face.py\n+++ b/src/align/detect_face.py\n@@ -85,10 +85,10 @@ class Network(object):\n         data_dict = np.load(data_path, encoding=\'latin1\').item() #pylint: disable=no-member\n \n         for op_name in data_dict:\n-            with tf.variable_scope(op_name, reuse=True):\n+            with tf.compat.v1.variable_scope(op_name, reuse=True):\n                 for param_name, data in iteritems(data_dict[op_name]):\n                     try:\n-                        var = tf.get_variable(param_name)\n+                        var = tf.compat.v1.get_variable(param_name)\n                         session.run(var.assign(data))\n                     except ValueError:\n                         if not ignore_missing:\n@@ -122,7 +122,7 @@ class Network(object):\n \n     def make_var(self, name, shape):\n         """Creates a new TensorFlow variable."""\n-        return tf.get_variable(name, shape, trainable=self.trainable)\n+        return tf.compat.v1.get_variable(name, shape, trainable=self.trainable)\n \n     def validate_padding(self, padding):\n         """Verifies that the padding is one of the supported ones."""\n@@ -149,8 +149,8 @@ class Network(object):\n         assert c_i % group == 0\n         assert c_o % group == 0\n         # Convolution for a given input and kernel\n-        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n-        with tf.variable_scope(name) as scope:\n+        convolve = lambda i, k: tf.nn.conv2d(input=i, filters=k, strides=[1, s_h, s_w, 1], padding=padding)\n+        with tf.compat.v1.variable_scope(name) as scope:\n             kernel = self.make_var(\'weights\', shape=[k_h, k_w, c_i // group, c_o])\n             # This is the common-case. Convolve the input without any further complications.\n             output = convolve(inp, kernel)\n@@ -165,7 +165,7 @@ class Network(object):\n \n     @layer\n     def prelu(self, inp, name):\n-        with tf.variable_scope(name):\n+        with tf.compat.v1.variable_scope(name):\n             i = int(inp.get_shape()[-1])\n             alpha = self.make_var(\'alpha\', shape=(i,))\n             output = tf.nn.relu(inp) + tf.multiply(alpha, -tf.nn.relu(-inp))\n@@ -174,7 +174,7 @@ class Network(object):\n     @layer\n     def max_pool(self, inp, k_h, k_w, s_h, s_w, name, padding=\'SAME\'):\n         self.validate_padding(padding)\n-        return tf.nn.max_pool(inp,\n+        return tf.nn.max_pool2d(input=inp,\n                               ksize=[1, k_h, k_w, 1],\n                               strides=[1, s_h, s_w, 1],\n                               padding=padding,\n@@ -182,7 +182,7 @@ class Network(object):\n \n     @layer\n     def fc(self, inp, num_out, name, relu=True):\n-        with tf.variable_scope(name):\n+        with tf.compat.v1.variable_scope(name):\n             input_shape = inp.get_shape()\n             if input_shape.ndims == 4:\n                 # The input is spatial. Vectorize it first.\n@@ -194,7 +194,7 @@ class Network(object):\n                 feed_in, dim = (inp, input_shape[-1].value)\n             weights = self.make_var(\'weights\', shape=[dim, num_out])\n             biases = self.make_var(\'biases\', [num_out])\n-            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n+            op = tf.compat.v1.nn.relu_layer if relu else tf.compat.v1.nn.xw_plus_b\n             fc = op(feed_in, weights, biases, name=name)\n             return fc\n \n@@ -207,10 +207,10 @@ class Network(object):\n     """\n     @layer\n     def softmax(self, target, axis, name=None):\n-        max_axis = tf.reduce_max(target, axis, keepdims=True)\n+        max_axis = tf.reduce_max(input_tensor=target, axis=axis, keepdims=True)\n         target_exp = tf.exp(target-max_axis)\n-        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)\n-        softmax = tf.div(target_exp, normalize, name)\n+        normalize = tf.reduce_sum(input_tensor=target_exp, axis=axis, keepdims=True)\n+        softmax = tf.compat.v1.div(target_exp, normalize, name)\n         return softmax\n     \n class PNet(Network):\n@@ -277,16 +277,16 @@ def create_mtcnn(sess, model_path):\n     if not model_path:\n         model_path,_ = os.path.split(os.path.realpath(__file__))\n \n-    with tf.variable_scope(\'pnet\'):\n-        data = tf.placeholder(tf.float32, (None,None,None,3), \'input\')\n+    with tf.compat.v1.variable_scope(\'pnet\'):\n+        data = tf.compat.v1.placeholder(tf.float32, (None,None,None,3), \'input\')\n         pnet = PNet({\'data\':data})\n         pnet.load(os.path.join(model_path, \'det1.npy\'), sess)\n-    with tf.variable_scope(\'rnet\'):\n-        data = tf.placeholder(tf.float32, (None,24,24,3), \'input\')\n+    with tf.compat.v1.variable_scope(\'rnet\'):\n+        data = tf.compat.v1.placeholder(tf.float32, (None,24,24,3), \'input\')\n         rnet = RNet({\'data\':data})\n         rnet.load(os.path.join(model_path, \'det2.npy\'), sess)\n-    with tf.variable_scope(\'onet\'):\n-        data = tf.placeholder(tf.float32, (None,48,48,3), \'input\')\n+    with tf.compat.v1.variable_scope(\'onet\'):\n+        data = tf.compat.v1.placeholder(tf.float32, (None,48,48,3), \'input\')\n         onet = ONet({\'data\':data})\n         onet.load(os.path.join(model_path, \'det3.npy\'), sess)\n         \ndiff --git a/src/calculate_filtering_metrics.py b/src/calculate_filtering_metrics.py\nindex f60b9ae..9916fa7 100644\n--- a/src/calculate_filtering_metrics.py\n+++ b/src/calculate_filtering_metrics.py\n@@ -54,15 +54,15 @@ def main(args):\n         \n         model_exp = os.path.expanduser(args.model_file)\n         with gfile.FastGFile(model_exp,\'rb\') as f:\n-            graph_def = tf.GraphDef()\n+            graph_def = tf.compat.v1.GraphDef()\n             graph_def.ParseFromString(f.read())\n             input_map={\'input\':image_batch, \'phase_train\':False}\n             tf.import_graph_def(graph_def, input_map=input_map, name=\'net\')\n         \n-        embeddings = tf.get_default_graph().get_tensor_by_name("net/embeddings:0")\n+        embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("net/embeddings:0")\n \n-        with tf.Session() as sess:\n-            tf.train.start_queue_runners(sess=sess)\n+        with tf.compat.v1.Session() as sess:\n+            tf.compat.v1.train.start_queue_runners(sess=sess)\n                 \n             embedding_size = int(embeddings.get_shape()[1])\n             nrof_batches = int(math.ceil(nrof_images / args.batch_size))\ndiff --git a/src/classifier.py b/src/classifier.py\nindex 749db4d..844b55e 100644\n--- a/src/classifier.py\n+++ b/src/classifier.py\n@@ -40,7 +40,7 @@ def main(args):\n   \n     with tf.Graph().as_default():\n       \n-        with tf.Session() as sess:\n+        with tf.compat.v1.Session() as sess:\n             \n             np.random.seed(seed=args.seed)\n             \n@@ -69,9 +69,9 @@ def main(args):\n             facenet.load_model(args.model)\n             \n             # Get input and output tensors\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n             embedding_size = embeddings.get_shape()[1]\n             \n             # Run forward pass to calculate embeddings\ndiff --git a/src/compare.py b/src/compare.py\nindex bc53cc4..abf1fce 100644\n--- a/src/compare.py\n+++ b/src/compare.py\n@@ -41,15 +41,15 @@ def main(args):\n     images = load_and_align_data(args.image_files, args.image_size, args.margin, args.gpu_memory_fraction)\n     with tf.Graph().as_default():\n \n-        with tf.Session() as sess:\n+        with tf.compat.v1.Session() as sess:\n       \n             # Load the model\n             facenet.load_model(args.model)\n     \n             # Get input and output tensors\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n \n             # Run forward pass to calculate embeddings\n             feed_dict = { images_placeholder: images, phase_train_placeholder:False }\n@@ -84,8 +84,8 @@ def load_and_align_data(image_paths, image_size, margin, gpu_memory_fraction):\n     \n     print(\'Creating networks and loading parameters\')\n     with tf.Graph().as_default():\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n+        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n         with sess.as_default():\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n   \ndiff --git a/src/facenet.py b/src/facenet.py\nindex 0e05676..8e64de7 100644\n--- a/src/facenet.py\n+++ b/src/facenet.py\n@@ -52,12 +52,12 @@ def triplet_loss(anchor, positive, negative, alpha):\n     Returns:\n       the triplet loss according to the FaceNet paper as a float tensor.\n     """\n-    with tf.variable_scope(\'triplet_loss\'):\n-        pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n-        neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n+    with tf.compat.v1.variable_scope(\'triplet_loss\'):\n+        pos_dist = tf.reduce_sum(input_tensor=tf.square(tf.subtract(anchor, positive)), axis=1)\n+        neg_dist = tf.reduce_sum(input_tensor=tf.square(tf.subtract(anchor, negative)), axis=1)\n         \n         basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\n-        loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n+        loss = tf.reduce_mean(input_tensor=tf.maximum(basic_loss, 0.0), axis=0)\n       \n     return loss\n   \n@@ -66,14 +66,14 @@ def center_loss(features, label, alfa, nrof_classes):\n        (http://ydwen.github.io/papers/WenECCV16.pdf)\n     """\n     nrof_features = features.get_shape()[1]\n-    centers = tf.get_variable(\'centers\', [nrof_classes, nrof_features], dtype=tf.float32,\n-        initializer=tf.constant_initializer(0), trainable=False)\n+    centers = tf.compat.v1.get_variable(\'centers\', [nrof_classes, nrof_features], dtype=tf.float32,\n+        initializer=tf.compat.v1.constant_initializer(0), trainable=False)\n     label = tf.reshape(label, [-1])\n     centers_batch = tf.gather(centers, label)\n     diff = (1 - alfa) * (centers_batch - features)\n-    centers = tf.scatter_sub(centers, label, diff)\n+    centers = tf.compat.v1.scatter_sub(centers, label, diff)\n     with tf.control_dependencies([centers]):\n-        loss = tf.reduce_mean(tf.square(features - centers_batch))\n+        loss = tf.reduce_mean(input_tensor=tf.square(features - centers_batch))\n     return loss, centers\n \n def get_image_paths_and_labels(dataset):\n@@ -106,29 +106,29 @@ def create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batc\n         filenames, label, control = input_queue.dequeue()\n         images = []\n         for filename in tf.unstack(filenames):\n-            file_contents = tf.read_file(filename)\n+            file_contents = tf.io.read_file(filename)\n             image = tf.image.decode_image(file_contents, 3)\n-            image = tf.cond(get_control_flag(control[0], RANDOM_ROTATE),\n-                            lambda:tf.py_func(random_rotate_image, [image], tf.uint8), \n-                            lambda:tf.identity(image))\n-            image = tf.cond(get_control_flag(control[0], RANDOM_CROP), \n-                            lambda:tf.random_crop(image, image_size + (3,)), \n-                            lambda:tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n-            image = tf.cond(get_control_flag(control[0], RANDOM_FLIP),\n-                            lambda:tf.image.random_flip_left_right(image),\n-                            lambda:tf.identity(image))\n-            image = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION),\n-                            lambda:(tf.cast(image, tf.float32) - 127.5)/128.0,\n-                            lambda:tf.image.per_image_standardization(image))\n-            image = tf.cond(get_control_flag(control[0], FLIP),\n-                            lambda:tf.image.flip_left_right(image),\n-                            lambda:tf.identity(image))\n+            image = tf.cond(pred=get_control_flag(control[0], RANDOM_ROTATE),\n+                            true_fn=lambda:tf.compat.v1.py_func(random_rotate_image, [image], tf.uint8), \n+                            false_fn=lambda:tf.identity(image))\n+            image = tf.cond(pred=get_control_flag(control[0], RANDOM_CROP), \n+                            true_fn=lambda:tf.image.random_crop(image, image_size + (3,)), \n+                            false_fn=lambda:tf.image.resize_with_crop_or_pad(image, image_size[0], image_size[1]))\n+            image = tf.cond(pred=get_control_flag(control[0], RANDOM_FLIP),\n+                            true_fn=lambda:tf.image.random_flip_left_right(image),\n+                            false_fn=lambda:tf.identity(image))\n+            image = tf.cond(pred=get_control_flag(control[0], FIXED_STANDARDIZATION),\n+                            true_fn=lambda:(tf.cast(image, tf.float32) - 127.5)/128.0,\n+                            false_fn=lambda:tf.image.per_image_standardization(image))\n+            image = tf.cond(pred=get_control_flag(control[0], FLIP),\n+                            true_fn=lambda:tf.image.flip_left_right(image),\n+                            false_fn=lambda:tf.identity(image))\n             #pylint: disable=no-member\n             image.set_shape(image_size + (3,))\n             images.append(image)\n         images_and_labels_list.append([images, label])\n \n-    image_batch, label_batch = tf.train.batch_join(\n+    image_batch, label_batch = tf.compat.v1.train.batch_join(\n         images_and_labels_list, batch_size=batch_size_placeholder, \n         shapes=[image_size + (3,), ()], enqueue_many=True,\n         capacity=4 * nrof_preprocess_threads * 100,\n@@ -137,7 +137,7 @@ def create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batc\n     return image_batch, label_batch\n \n def get_control_flag(control, field):\n-    return tf.equal(tf.mod(tf.floor_div(control, field), 2), 1)\n+    return tf.equal(tf.math.floormod(tf.math.floordiv(control, field), 2), 1)\n   \n def _add_loss_summaries(total_loss):\n     """Add summaries for losses.\n@@ -152,7 +152,7 @@ def _add_loss_summaries(total_loss):\n     """\n     # Compute the moving average of all individual losses and the total loss.\n     loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n-    losses = tf.get_collection(\'losses\')\n+    losses = tf.compat.v1.get_collection(\'losses\')\n     loss_averages_op = loss_averages.apply(losses + [total_loss])\n   \n     # Attach a scalar summmary to all individual losses and the total loss; do the\n@@ -160,8 +160,8 @@ def _add_loss_summaries(total_loss):\n     for l in losses + [total_loss]:\n         # Name each loss as \'(raw)\' and name the moving average version of the loss\n         # as the original loss name.\n-        tf.summary.scalar(l.op.name +\' (raw)\', l)\n-        tf.summary.scalar(l.op.name, loss_averages.average(l))\n+        tf.compat.v1.summary.scalar(l.op.name +\' (raw)\', l)\n+        tf.compat.v1.summary.scalar(l.op.name, loss_averages.average(l))\n   \n     return loss_averages_op\n \n@@ -172,15 +172,15 @@ def train(total_loss, global_step, optimizer, learning_rate, moving_average_deca\n     # Compute gradients.\n     with tf.control_dependencies([loss_averages_op]):\n         if optimizer==\'ADAGRAD\':\n-            opt = tf.train.AdagradOptimizer(learning_rate)\n+            opt = tf.compat.v1.train.AdagradOptimizer(learning_rate)\n         elif optimizer==\'ADADELTA\':\n-            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n+            opt = tf.compat.v1.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n         elif optimizer==\'ADAM\':\n-            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n+            opt = tf.compat.v1.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n         elif optimizer==\'RMSPROP\':\n-            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n+            opt = tf.compat.v1.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n         elif optimizer==\'MOM\':\n-            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n+            opt = tf.compat.v1.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n         else:\n             raise ValueError(\'Invalid optimization algorithm\')\n     \n@@ -191,19 +191,19 @@ def train(total_loss, global_step, optimizer, learning_rate, moving_average_deca\n   \n     # Add histograms for trainable variables.\n     if log_histograms:\n-        for var in tf.trainable_variables():\n-            tf.summary.histogram(var.op.name, var)\n+        for var in tf.compat.v1.trainable_variables():\n+            tf.compat.v1.summary.histogram(var.op.name, var)\n    \n     # Add histograms for gradients.\n     if log_histograms:\n         for grad, var in grads:\n             if grad is not None:\n-                tf.summary.histogram(var.op.name + \'/gradients\', grad)\n+                tf.compat.v1.summary.histogram(var.op.name + \'/gradients\', grad)\n   \n     # Track the moving averages of all trainable variables.\n     variable_averages = tf.train.ExponentialMovingAverage(\n         moving_average_decay, global_step)\n-    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n+    variables_averages_op = variable_averages.apply(tf.compat.v1.trainable_variables())\n   \n     with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n         train_op = tf.no_op(name=\'train\')\n@@ -368,7 +368,7 @@ def load_model(model, input_map=None):\n     if (os.path.isfile(model_exp)):\n         print(\'Model filename: %s\' % model_exp)\n         with gfile.FastGFile(model_exp,\'rb\') as f:\n-            graph_def = tf.GraphDef()\n+            graph_def = tf.compat.v1.GraphDef()\n             graph_def.ParseFromString(f.read())\n             tf.import_graph_def(graph_def, input_map=input_map, name=\'\')\n     else:\n@@ -378,8 +378,8 @@ def load_model(model, input_map=None):\n         print(\'Metagraph file: %s\' % meta_file)\n         print(\'Checkpoint file: %s\' % ckpt_file)\n       \n-        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n-        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n+        saver = tf.compat.v1.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n+        saver.restore(tf.compat.v1.get_default_session(), os.path.join(model_exp, ckpt_file))\n     \n def get_model_filenames(model_dir):\n     files = os.listdir(model_dir)\ndiff --git a/src/freeze_graph.py b/src/freeze_graph.py\nindex 3584c18..c59517d 100644\n--- a/src/freeze_graph.py\n+++ b/src/freeze_graph.py\n@@ -37,7 +37,7 @@ from six.moves import xrange  # @UnresolvedImport\n \n def main(args):\n     with tf.Graph().as_default():\n-        with tf.Session() as sess:\n+        with tf.compat.v1.Session() as sess:\n             # Load the model metagraph and checkpoint\n             print(\'Model directory: %s\' % args.model_dir)\n             meta_file, ckpt_file = facenet.get_model_filenames(os.path.expanduser(args.model_dir))\n@@ -46,10 +46,10 @@ def main(args):\n             print(\'Checkpoint file: %s\' % ckpt_file)\n \n             model_dir_exp = os.path.expanduser(args.model_dir)\n-            saver = tf.train.import_meta_graph(os.path.join(model_dir_exp, meta_file), clear_devices=True)\n-            tf.get_default_session().run(tf.global_variables_initializer())\n-            tf.get_default_session().run(tf.local_variables_initializer())\n-            saver.restore(tf.get_default_session(), os.path.join(model_dir_exp, ckpt_file))\n+            saver = tf.compat.v1.train.import_meta_graph(os.path.join(model_dir_exp, meta_file), clear_devices=True)\n+            tf.compat.v1.get_default_session().run(tf.compat.v1.global_variables_initializer())\n+            tf.compat.v1.get_default_session().run(tf.compat.v1.local_variables_initializer())\n+            saver.restore(tf.compat.v1.get_default_session(), os.path.join(model_dir_exp, ckpt_file))\n             \n             # Retrieve the protobuf graph definition and fix the batch norm nodes\n             input_graph_def = sess.graph.as_graph_def()\n@@ -58,7 +58,7 @@ def main(args):\n             output_graph_def = freeze_graph_def(sess, input_graph_def, \'embeddings,label_batch\')\n \n         # Serialize and dump the output graph to the filesystem\n-        with tf.gfile.GFile(args.output_file, \'wb\') as f:\n+        with tf.io.gfile.GFile(args.output_file, \'wb\') as f:\n             f.write(output_graph_def.SerializeToString())\n         print("%d ops in the final graph: %s" % (len(output_graph_def.node), args.output_file))\n         \ndiff --git a/src/generative/calculate_attribute_vectors.py b/src/generative/calculate_attribute_vectors.py\nindex 8fe3ead..925340f 100644\n--- a/src/generative/calculate_attribute_vectors.py\n+++ b/src/generative/calculate_attribute_vectors.py\n@@ -53,7 +53,7 @@ def main(args):\n     gen_image_size = vae.get_image_size()\n \n     with tf.Graph().as_default():\n-        tf.set_random_seed(args.seed)\n+        tf.compat.v1.set_random_seed(args.seed)\n         \n         image_list = facenet.get_image_paths(os.path.expanduser(args.data_dir))\n         \n@@ -68,15 +68,15 @@ def main(args):\n             \n         # Create the input queue\n         index_list = range(len(image_list))\n-        input_queue = tf.train.slice_input_producer([image_list, attribs_list, index_list], num_epochs=1, shuffle=False)        \n+        input_queue = tf.compat.v1.train.slice_input_producer([image_list, attribs_list, index_list], num_epochs=1, shuffle=False)        \n         \n         nrof_preprocess_threads = 4\n         image_per_thread = []\n         for _ in range(nrof_preprocess_threads):\n             filename = input_queue[0]\n-            file_contents = tf.read_file(filename)\n+            file_contents = tf.io.read_file(filename)\n             image = tf.image.decode_image(file_contents, channels=3)\n-            image = tf.image.resize_image_with_crop_or_pad(image, 160, 160)\n+            image = tf.image.resize_with_crop_or_pad(image, 160, 160)\n             #image = tf.image.resize_images(image, (64,64))\n             image.set_shape((args.image_size, args.image_size, 3))\n             attrib = input_queue[1]\n@@ -84,7 +84,7 @@ def main(args):\n             image = tf.cast(image, tf.float32)\n             image_per_thread.append([image, attrib, input_queue[2]])\n     \n-        images, attribs, indices = tf.train.batch_join(\n+        images, attribs, indices = tf.compat.v1.train.batch_join(\n             image_per_thread, batch_size=args.batch_size, \n             shapes=[(args.image_size, args.image_size, 3), (nrof_attributes,), ()], enqueue_many=False,\n             capacity=4 * nrof_preprocess_threads * args.batch_size,\n@@ -94,26 +94,26 @@ def main(args):\n         images_norm = (images-img_mean) / img_stddev\n \n         # Resize to appropriate size for the encoder \n-        images_norm_resize = tf.image.resize_images(images_norm, (gen_image_size,gen_image_size))\n+        images_norm_resize = tf.image.resize(images_norm, (gen_image_size,gen_image_size))\n         \n         # Create encoder network\n         mean, log_variance = vae.encoder(images_norm_resize, True)\n         \n-        epsilon = tf.random_normal((tf.shape(mean)[0], args.latent_var_size))\n+        epsilon = tf.random.normal((tf.shape(input=mean)[0], args.latent_var_size))\n         std = tf.exp(log_variance/2)\n         latent_var = mean + epsilon * std\n         \n         # Create a saver\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n+        saver = tf.compat.v1.train.Saver(tf.compat.v1.trainable_variables(), max_to_keep=3)\n         \n         # Start running operations on the Graph\n         gpu_memory_fraction = 1.0\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-        sess.run(tf.global_variables_initializer())\n-        sess.run(tf.local_variables_initializer())\n+        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n+        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        sess.run(tf.compat.v1.global_variables_initializer())\n+        sess.run(tf.compat.v1.local_variables_initializer())\n         coord = tf.train.Coordinator()\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\n+        tf.compat.v1.train.start_queue_runners(coord=coord, sess=sess)\n         \n \n         with sess.as_default():\ndiff --git a/src/generative/models/dfc_vae.py b/src/generative/models/dfc_vae.py\nindex b4450f2..eadf075 100644\n--- a/src/generative/models/dfc_vae.py\n+++ b/src/generative/models/dfc_vae.py\n@@ -30,7 +30,7 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n+import tf_slim as slim\n import generative.models.vae_base  # @UnresolvedImport\n \n \n@@ -42,12 +42,12 @@ class Vae(generative.models.vae_base.Vae):\n     def encoder(self, images, is_training):\n         activation_fn = leaky_relu  # tf.nn.relu\n         weight_decay = 0.0\n-        with tf.variable_scope(\'encoder\'):\n+        with tf.compat.v1.variable_scope(\'encoder\'):\n             with slim.arg_scope([slim.batch_norm],\n                                 is_training=is_training):\n                 with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n+                                    weights_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1),\n+                                    weights_regularizer=tf.keras.regularizers.l2(0.5 * (weight_decay)),\n                                     normalizer_fn=slim.batch_norm,\n                                     normalizer_params=self.batch_norm_params):\n                     net = slim.conv2d(images, 32, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_1\')\n@@ -62,31 +62,30 @@ class Vae(generative.models.vae_base.Vae):\n     def decoder(self, latent_var, is_training):\n         activation_fn = leaky_relu  # tf.nn.relu\n         weight_decay = 0.0 \n-        with tf.variable_scope(\'decoder\'):\n+        with tf.compat.v1.variable_scope(\'decoder\'):\n             with slim.arg_scope([slim.batch_norm],\n                                 is_training=is_training):\n                 with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n+                                    weights_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1),\n+                                    weights_regularizer=tf.keras.regularizers.l2(0.5 * (weight_decay)),\n                                     normalizer_fn=slim.batch_norm,\n                                     normalizer_params=self.batch_norm_params):\n                     net = slim.fully_connected(latent_var, 4096, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n                     net = tf.reshape(net, [-1,4,4,256], name=\'Reshape\')\n                     \n-                    net = tf.image.resize_nearest_neighbor(net, size=(8,8), name=\'Upsample_1\')\n+                    net = tf.image.resize(net, size=(8,8), name=\'Upsample_1\', method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                     net = slim.conv2d(net, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1\')\n             \n-                    net = tf.image.resize_nearest_neighbor(net, size=(16,16), name=\'Upsample_2\')\n+                    net = tf.image.resize(net, size=(16,16), name=\'Upsample_2\', method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                     net = slim.conv2d(net, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2\')\n             \n-                    net = tf.image.resize_nearest_neighbor(net, size=(32,32), name=\'Upsample_3\')\n+                    net = tf.image.resize(net, size=(32,32), name=\'Upsample_3\', method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                     net = slim.conv2d(net, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3\')\n             \n-                    net = tf.image.resize_nearest_neighbor(net, size=(64,64), name=\'Upsample_4\')\n+                    net = tf.image.resize(net, size=(64,64), name=\'Upsample_4\', method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                     net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=None, scope=\'Conv2d_4\')\n                 \n         return net\n       \n def leaky_relu(x):\n     return tf.maximum(0.1*x,x)\n-  \n\\ No newline at end of file\ndiff --git a/src/generative/models/dfc_vae_large.py b/src/generative/models/dfc_vae_large.py\nindex aa8e8b7..268f379 100644\n--- a/src/generative/models/dfc_vae_large.py\n+++ b/src/generative/models/dfc_vae_large.py\n@@ -30,7 +30,7 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n+import tf_slim as slim\n import generative.models.vae_base  # @UnresolvedImport\n \n \n@@ -43,12 +43,12 @@ class Vae(generative.models.vae_base.Vae):\n     def encoder(self, images, is_training):\n         activation_fn = leaky_relu  # tf.nn.relu\n         weight_decay = 0.0\n-        with tf.variable_scope(\'encoder\'):\n+        with tf.compat.v1.variable_scope(\'encoder\'):\n             with slim.arg_scope([slim.batch_norm],\n                                 is_training=is_training):\n                 with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n+                                    weights_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1),\n+                                    weights_regularizer=tf.keras.regularizers.l2(0.5 * (weight_decay)),\n                                     normalizer_fn=slim.batch_norm,\n                                     normalizer_params=self.batch_norm_params):\n                     net = slim.conv2d(images, 32, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_1\')\n@@ -64,30 +64,30 @@ class Vae(generative.models.vae_base.Vae):\n     def decoder(self, latent_var, is_training):\n         activation_fn = leaky_relu  # tf.nn.relu\n         weight_decay = 0.0 \n-        with tf.variable_scope(\'decoder\'):\n+        with tf.compat.v1.variable_scope(\'decoder\'):\n             with slim.arg_scope([slim.batch_norm],\n                                 is_training=is_training):\n                 with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n+                                    weights_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1),\n+                                    weights_regularizer=tf.keras.regularizers.l2(0.5 * (weight_decay)),\n                                     normalizer_fn=slim.batch_norm,\n                                     normalizer_params=self.batch_norm_params):\n                     net = slim.fully_connected(latent_var, 4096, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n                     net = tf.reshape(net, [-1,4,4,256], name=\'Reshape\')\n                     \n-                    net = tf.image.resize_nearest_neighbor(net, size=(8,8), name=\'Upsample_1\')\n+                    net = tf.image.resize(net, size=(8,8), name=\'Upsample_1\', method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                     net = slim.conv2d(net, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1\')\n             \n-                    net = tf.image.resize_nearest_neighbor(net, size=(16,16), name=\'Upsample_2\')\n+                    net = tf.image.resize(net, size=(16,16), name=\'Upsample_2\', method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                     net = slim.conv2d(net, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2\')\n             \n-                    net = tf.image.resize_nearest_neighbor(net, size=(32,32), name=\'Upsample_3\')\n+                    net = tf.image.resize(net, size=(32,32), name=\'Upsample_3\', method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                     net = slim.conv2d(net, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3\')\n             \n-                    net = tf.image.resize_nearest_neighbor(net, size=(64,64), name=\'Upsample_4\')\n+                    net = tf.image.resize(net, size=(64,64), name=\'Upsample_4\', method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                     net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_4\')\n                 \n-                    net = tf.image.resize_nearest_neighbor(net, size=(128,128), name=\'Upsample_5\')\n+                    net = tf.image.resize(net, size=(128,128), name=\'Upsample_5\', method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                     net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=None, scope=\'Conv2d_5\')\n         return net\n \ndiff --git a/src/generative/models/dfc_vae_resnet.py b/src/generative/models/dfc_vae_resnet.py\nindex 7c2f52c..6c57c02 100644\n--- a/src/generative/models/dfc_vae_resnet.py\n+++ b/src/generative/models/dfc_vae_resnet.py\n@@ -30,7 +30,7 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n+import tf_slim as slim\n import generative.models.vae_base  # @UnresolvedImport\n \n \n@@ -42,12 +42,12 @@ class Vae(generative.models.vae_base.Vae):\n     def encoder(self, images, is_training):\n         activation_fn = leaky_relu  # tf.nn.relu\n         weight_decay = 0.0\n-        with tf.variable_scope(\'encoder\'):\n+        with tf.compat.v1.variable_scope(\'encoder\'):\n             with slim.arg_scope([slim.batch_norm],\n                                 is_training=is_training):\n                 with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n+                                    weights_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1),\n+                                    weights_regularizer=tf.keras.regularizers.l2(0.5 * (weight_decay)),\n                                     normalizer_fn=slim.batch_norm,\n                                     normalizer_params=self.batch_norm_params):\n                     net = images\n@@ -72,30 +72,30 @@ class Vae(generative.models.vae_base.Vae):\n     def decoder(self, latent_var, is_training):\n         activation_fn = leaky_relu  # tf.nn.relu\n         weight_decay = 0.0 \n-        with tf.variable_scope(\'decoder\'):\n+        with tf.compat.v1.variable_scope(\'decoder\'):\n             with slim.arg_scope([slim.batch_norm],\n                                 is_training=is_training):\n                 with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n+                                    weights_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1),\n+                                    weights_regularizer=tf.keras.regularizers.l2(0.5 * (weight_decay)),\n                                     normalizer_fn=slim.batch_norm,\n                                     normalizer_params=self.batch_norm_params):\n                     net = slim.fully_connected(latent_var, 4096, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n                     net = tf.reshape(net, [-1,4,4,256], name=\'Reshape\')\n                     \n-                    net = tf.image.resize_nearest_neighbor(net, size=(8,8), name=\'Upsample_1\')\n+                    net = tf.image.resize(net, size=(8,8), name=\'Upsample_1\', method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                     net = slim.conv2d(net, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1a\')\n                     net = slim.repeat(net, 3, conv2d_block, 0.1, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1b\')\n             \n-                    net = tf.image.resize_nearest_neighbor(net, size=(16,16), name=\'Upsample_2\')\n+                    net = tf.image.resize(net, size=(16,16), name=\'Upsample_2\', method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                     net = slim.conv2d(net, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2a\')\n                     net = slim.repeat(net, 3, conv2d_block, 0.1, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2b\')\n             \n-                    net = tf.image.resize_nearest_neighbor(net, size=(32,32), name=\'Upsample_3\')\n+                    net = tf.image.resize(net, size=(32,32), name=\'Upsample_3\', method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                     net = slim.conv2d(net, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3a\')\n                     net = slim.repeat(net, 3, conv2d_block, 0.1, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3b\')\n             \n-                    net = tf.image.resize_nearest_neighbor(net, size=(64,64), name=\'Upsample_4\')\n+                    net = tf.image.resize(net, size=(64,64), name=\'Upsample_4\', method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                     net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_4a\')\n                     net = slim.repeat(net, 3, conv2d_block, 0.1, 3, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_4b\')\n                     net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=None, scope=\'Conv2d_4c\')\n@@ -107,4 +107,3 @@ def conv2d_block(inp, scale, *args, **kwargs):\n \n def leaky_relu(x):\n     return tf.maximum(0.1*x,x)\n-  \n\\ No newline at end of file\ndiff --git a/src/generative/models/vae_base.py b/src/generative/models/vae_base.py\nindex 7437251..640faf2 100644\n--- a/src/generative/models/vae_base.py\n+++ b/src/generative/models/vae_base.py\n@@ -42,7 +42,7 @@ class Vae(object):\n         # force in-place updates of mean and variance estimates\n         \'updates_collections\': None,\n         # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n+        \'variables_collections\': [ tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES ],\n     }\n   \n     def encoder(self, images, is_training):\n@@ -55,4 +55,3 @@ class Vae(object):\n \n     def get_image_size(self):\n         return self.image_size\n-        \n\\ No newline at end of file\ndiff --git a/src/generative/modify_attribute.py b/src/generative/modify_attribute.py\nindex 8187cff..37280ea 100644\n--- a/src/generative/modify_attribute.py\n+++ b/src/generative/modify_attribute.py\n@@ -49,20 +49,20 @@ def main(args):\n     gen_image_size = vae.get_image_size()\n \n     with tf.Graph().as_default():\n-        tf.set_random_seed(args.seed)\n+        tf.compat.v1.set_random_seed(args.seed)\n         \n-        images = tf.placeholder(tf.float32, shape=(None,gen_image_size,gen_image_size,3), name=\'input\')\n+        images = tf.compat.v1.placeholder(tf.float32, shape=(None,gen_image_size,gen_image_size,3), name=\'input\')\n         \n         # Normalize\n         images_norm = (images-img_mean) / img_stddev\n \n         # Resize to appropriate size for the encoder \n-        images_norm_resize = tf.image.resize_images(images_norm, (gen_image_size,gen_image_size))\n+        images_norm_resize = tf.image.resize(images_norm, (gen_image_size,gen_image_size))\n         \n         # Create encoder network\n         mean, log_variance = vae.encoder(images_norm_resize, True)\n         \n-        epsilon = tf.random_normal((tf.shape(mean)[0], args.latent_var_size))\n+        epsilon = tf.random.normal((tf.shape(input=mean)[0], args.latent_var_size))\n         std = tf.exp(log_variance/2)\n         latent_var = mean + epsilon * std\n         \n@@ -73,16 +73,16 @@ def main(args):\n         reconstructed = (reconstructed_norm*img_stddev) + img_mean\n \n         # Create a saver\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n+        saver = tf.compat.v1.train.Saver(tf.compat.v1.trainable_variables(), max_to_keep=3)\n         \n         # Start running operations on the Graph\n         gpu_memory_fraction = 1.0\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-        sess.run(tf.global_variables_initializer())\n-        sess.run(tf.local_variables_initializer())\n+        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n+        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        sess.run(tf.compat.v1.global_variables_initializer())\n+        sess.run(tf.compat.v1.local_variables_initializer())\n         coord = tf.train.Coordinator()\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\n+        tf.compat.v1.train.start_queue_runners(coord=coord, sess=sess)\n         \n \n         with sess.as_default():\ndiff --git a/src/generative/train_vae.py b/src/generative/train_vae.py\nindex c3c882f..78ff833 100644\n--- a/src/generative/train_vae.py\n+++ b/src/generative/train_vae.py\n@@ -27,7 +27,7 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n+import tf_slim as slim\n import sys\n import time\n import importlib\n@@ -63,27 +63,27 @@ def main(args):\n     facenet.store_revision_info(src_path, model_dir, \' \'.join(sys.argv))\n     \n     with tf.Graph().as_default():\n-        tf.set_random_seed(args.seed)\n+        tf.compat.v1.set_random_seed(args.seed)\n         global_step = tf.Variable(0, trainable=False)\n         \n         train_set = facenet.get_dataset(args.data_dir)\n         image_list, _ = facenet.get_image_paths_and_labels(train_set)\n         \n         # Create the input queue\n-        input_queue = tf.train.string_input_producer(image_list, shuffle=True)\n+        input_queue = tf.compat.v1.train.string_input_producer(image_list, shuffle=True)\n     \n         nrof_preprocess_threads = 4\n         image_per_thread = []\n         for _ in range(nrof_preprocess_threads):\n-            file_contents = tf.read_file(input_queue.dequeue())\n+            file_contents = tf.io.read_file(input_queue.dequeue())\n             image = tf.image.decode_image(file_contents, channels=3)\n-            image = tf.image.resize_image_with_crop_or_pad(image, args.input_image_size, args.input_image_size)\n+            image = tf.image.resize_with_crop_or_pad(image, args.input_image_size, args.input_image_size)\n             image.set_shape((args.input_image_size, args.input_image_size, 3))\n             image = tf.cast(image, tf.float32)\n             #pylint: disable=no-member\n             image_per_thread.append([image])\n     \n-        images = tf.train.batch_join(\n+        images = tf.compat.v1.train.batch_join(\n             image_per_thread, batch_size=args.batch_size,\n             capacity=4 * nrof_preprocess_threads * args.batch_size,\n             allow_smaller_final_batch=False)\n@@ -92,12 +92,12 @@ def main(args):\n         images_norm = (images-img_mean) / img_stddev\n \n         # Resize to appropriate size for the encoder \n-        images_norm_resize = tf.image.resize_images(images_norm, (gen_image_size,gen_image_size))\n+        images_norm_resize = tf.image.resize(images_norm, (gen_image_size,gen_image_size))\n         \n         # Create encoder network\n         mean, log_variance = vae.encoder(images_norm_resize, True)\n         \n-        epsilon = tf.random_normal((tf.shape(mean)[0], args.latent_var_size))\n+        epsilon = tf.random.normal((tf.shape(input=mean)[0], args.latent_var_size))\n         std = tf.exp(log_variance/2)\n         latent_var = mean + epsilon * std\n         \n@@ -109,12 +109,12 @@ def main(args):\n         \n         # Create reconstruction loss\n         if args.reconstruction_loss_type==\'PLAIN\':\n-            images_resize = tf.image.resize_images(images, (gen_image_size,gen_image_size))\n-            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.pow(images_resize - reconstructed,2)))\n+            images_resize = tf.image.resize(images, (gen_image_size,gen_image_size))\n+            reconstruction_loss = tf.reduce_mean(input_tensor=tf.reduce_sum(input_tensor=tf.pow(images_resize - reconstructed,2)))\n         elif args.reconstruction_loss_type==\'PERCEPTUAL\':\n             network = importlib.import_module(args.model_def)\n \n-            reconstructed_norm_resize = tf.image.resize_images(reconstructed_norm, (args.input_image_size,args.input_image_size))\n+            reconstructed_norm_resize = tf.image.resize(reconstructed_norm, (args.input_image_size,args.input_image_size))\n \n             # Stack images from both the input batch and the reconstructed batch in a new tensor \n             shp = [-1] + images_norm.get_shape().as_list()[1:]\n@@ -130,7 +130,7 @@ def main(args):\n             for feature_name in feature_names:\n                 feature_flat = slim.flatten(end_points[feature_name])\n                 image_feature, reconstructed_feature = tf.unstack(tf.reshape(feature_flat, [2,args.batch_size,-1]), num=2, axis=0)\n-                reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.pow(image_feature-reconstructed_feature, 2)), name=feature_name+\'_loss\')\n+                reconstruction_loss = tf.reduce_mean(input_tensor=tf.reduce_sum(input_tensor=tf.pow(image_feature-reconstructed_feature, 2)), name=feature_name+\'_loss\')\n                 reconstruction_loss_list.append(reconstruction_loss)\n             # Sum up the losses in for the different features\n             reconstruction_loss = tf.add_n(reconstruction_loss_list, \'reconstruction_loss\')\n@@ -139,15 +139,15 @@ def main(args):\n         \n         # Create KL divergence loss\n         kl_loss = kl_divergence_loss(mean, log_variance)\n-        kl_loss_mean = tf.reduce_mean(kl_loss)\n+        kl_loss_mean = tf.reduce_mean(input_tensor=kl_loss)\n         \n         total_loss = args.alfa*kl_loss_mean + args.beta*reconstruction_loss\n         \n-        learning_rate = tf.train.exponential_decay(args.initial_learning_rate, global_step,\n+        learning_rate = tf.compat.v1.train.exponential_decay(args.initial_learning_rate, global_step,\n             args.learning_rate_decay_steps, args.learning_rate_decay_factor, staircase=True)\n         \n         # Calculate gradients and make sure not to include parameters for the perceptual loss model\n-        opt = tf.train.AdamOptimizer(learning_rate)\n+        opt = tf.compat.v1.train.AdamOptimizer(learning_rate)\n         grads = opt.compute_gradients(total_loss, var_list=get_variables_to_train())\n         \n         # Apply gradients\n@@ -156,18 +156,18 @@ def main(args):\n             train_op = tf.no_op(name=\'train\')\n \n         # Create a saver\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n+        saver = tf.compat.v1.train.Saver(tf.compat.v1.trainable_variables(), max_to_keep=3)\n         \n-        facenet_saver = tf.train.Saver(get_facenet_variables_to_restore())\n+        facenet_saver = tf.compat.v1.train.Saver(get_facenet_variables_to_restore())\n \n         # Start running operations on the Graph\n         gpu_memory_fraction = 1.0\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-        sess.run(tf.global_variables_initializer())\n-        sess.run(tf.local_variables_initializer())\n+        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n+        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        sess.run(tf.compat.v1.global_variables_initializer())\n+        sess.run(tf.compat.v1.local_variables_initializer())\n         coord = tf.train.Coordinator()\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\n+        tf.compat.v1.train.start_queue_runners(coord=coord, sess=sess)\n \n         with sess.as_default():\n             \n@@ -218,21 +218,21 @@ def main(args):\n \n def get_variables_to_train():\n     train_variables = []\n-    for var in tf.trainable_variables():\n+    for var in tf.compat.v1.trainable_variables():\n         if \'Inception\' not in var.name:\n             train_variables.append(var)\n     return train_variables\n \n def get_facenet_variables_to_restore():\n     facenet_variables = []\n-    for var in tf.global_variables():\n+    for var in tf.compat.v1.global_variables():\n         if var.name.startswith(\'Inception\'):\n             if \'Adam\' not in var.name:\n                 facenet_variables.append(var)\n     return facenet_variables\n \n def kl_divergence_loss(mean, log_variance):\n-    kl = 0.5 * tf.reduce_sum( tf.exp(log_variance) + tf.square(mean) - 1.0 - log_variance, reduction_indices = 1)\n+    kl = 0.5 * tf.reduce_sum( input_tensor=tf.exp(log_variance) + tf.square(mean) - 1.0 - log_variance, axis = 1)\n     return kl\n \n def parse_arguments(argv):\ndiff --git a/src/models/dummy.py b/src/models/dummy.py\nindex 7afe1ef..390b7d5 100644\n--- a/src/models/dummy.py\n+++ b/src/models/dummy.py\n@@ -27,7 +27,7 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n+import tf_slim as slim\n import numpy as np\n   \n def inference(images, keep_probability, phase_train=True,  # @UnusedVariable\n@@ -40,12 +40,12 @@ def inference(images, keep_probability, phase_train=True,  # @UnusedVariable\n         # force in-place updates of mean and variance estimates\n         \'updates_collections\': None,\n         # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n+        \'variables_collections\': [ tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES ],\n     }\n     \n     with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n+                        weights_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1),\n+                        weights_regularizer=tf.keras.regularizers.l2(0.5 * (weight_decay)),\n                         normalizer_fn=slim.batch_norm,\n                         normalizer_params=batch_norm_params):\n         size = np.prod(images.get_shape()[1:].as_list())\ndiff --git a/src/models/inception_resnet_v1.py b/src/models/inception_resnet_v1.py\nindex 475e81b..eb0ec1f 100644\n--- a/src/models/inception_resnet_v1.py\n+++ b/src/models/inception_resnet_v1.py\n@@ -24,18 +24,18 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n+import tf_slim as slim\n \n # Inception-Resnet-A\n def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n     """Builds the 35x35 resnet block."""\n-    with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n+    with tf.compat.v1.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n+        with tf.compat.v1.variable_scope(\'Branch_0\'):\n             tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n+        with tf.compat.v1.variable_scope(\'Branch_1\'):\n             tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n             tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n-        with tf.variable_scope(\'Branch_2\'):\n+        with tf.compat.v1.variable_scope(\'Branch_2\'):\n             tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n             tower_conv2_1 = slim.conv2d(tower_conv2_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n             tower_conv2_2 = slim.conv2d(tower_conv2_1, 32, 3, scope=\'Conv2d_0c_3x3\')\n@@ -50,10 +50,10 @@ def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n # Inception-Resnet-B\n def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n     """Builds the 17x17 resnet block."""\n-    with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n+    with tf.compat.v1.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n+        with tf.compat.v1.variable_scope(\'Branch_0\'):\n             tower_conv = slim.conv2d(net, 128, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n+        with tf.compat.v1.variable_scope(\'Branch_1\'):\n             tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n             tower_conv1_1 = slim.conv2d(tower_conv1_0, 128, [1, 7],\n                                         scope=\'Conv2d_0b_1x7\')\n@@ -71,10 +71,10 @@ def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n # Inception-Resnet-C\n def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n     """Builds the 8x8 resnet block."""\n-    with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n+    with tf.compat.v1.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n+        with tf.compat.v1.variable_scope(\'Branch_0\'):\n             tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n+        with tf.compat.v1.variable_scope(\'Branch_1\'):\n             tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n             tower_conv1_1 = slim.conv2d(tower_conv1_0, 192, [1, 3],\n                                         scope=\'Conv2d_0b_1x3\')\n@@ -89,38 +89,38 @@ def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n     return net\n   \n def reduction_a(net, k, l, m, n):\n-    with tf.variable_scope(\'Branch_0\'):\n+    with tf.compat.v1.variable_scope(\'Branch_0\'):\n         tower_conv = slim.conv2d(net, n, 3, stride=2, padding=\'VALID\',\n                                  scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_1\'):\n+    with tf.compat.v1.variable_scope(\'Branch_1\'):\n         tower_conv1_0 = slim.conv2d(net, k, 1, scope=\'Conv2d_0a_1x1\')\n         tower_conv1_1 = slim.conv2d(tower_conv1_0, l, 3,\n                                     scope=\'Conv2d_0b_3x3\')\n         tower_conv1_2 = slim.conv2d(tower_conv1_1, m, 3,\n                                     stride=2, padding=\'VALID\',\n                                     scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_2\'):\n+    with tf.compat.v1.variable_scope(\'Branch_2\'):\n         tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                      scope=\'MaxPool_1a_3x3\')\n     net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n     return net\n \n def reduction_b(net):\n-    with tf.variable_scope(\'Branch_0\'):\n+    with tf.compat.v1.variable_scope(\'Branch_0\'):\n         tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n         tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_1\'):\n+    with tf.compat.v1.variable_scope(\'Branch_1\'):\n         tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n         tower_conv1_1 = slim.conv2d(tower_conv1, 256, 3, stride=2,\n                                     padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_2\'):\n+    with tf.compat.v1.variable_scope(\'Branch_2\'):\n         tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n         tower_conv2_1 = slim.conv2d(tower_conv2, 256, 3,\n                                     scope=\'Conv2d_0b_3x3\')\n         tower_conv2_2 = slim.conv2d(tower_conv2_1, 256, 3, stride=2,\n                                     padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_3\'):\n+    with tf.compat.v1.variable_scope(\'Branch_3\'):\n         tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                      scope=\'MaxPool_1a_3x3\')\n     net = tf.concat([tower_conv_1, tower_conv1_1,\n@@ -137,12 +137,12 @@ def inference(images, keep_probability, phase_train=True,\n         # force in-place updates of mean and variance estimates\n         \'updates_collections\': None,\n         # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n+        \'variables_collections\': [ tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES ],\n     }\n     \n     with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                         weights_initializer=slim.initializers.xavier_initializer(), \n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n+                        weights_regularizer=tf.keras.regularizers.l2(0.5 * (weight_decay)),\n                         normalizer_fn=slim.batch_norm,\n                         normalizer_params=batch_norm_params):\n         return inception_resnet_v1(images, is_training=phase_train,\n@@ -169,7 +169,7 @@ def inception_resnet_v1(inputs, is_training=True,\n     """\n     end_points = {}\n   \n-    with tf.variable_scope(scope, \'InceptionResnetV1\', [inputs], reuse=reuse):\n+    with tf.compat.v1.variable_scope(scope, \'InceptionResnetV1\', [inputs], reuse=reuse):\n         with slim.arg_scope([slim.batch_norm, slim.dropout],\n                             is_training=is_training):\n             with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n@@ -208,7 +208,7 @@ def inception_resnet_v1(inputs, is_training=True,\n                 end_points[\'Mixed_5a\'] = net\n         \n                 # Reduction-A\n-                with tf.variable_scope(\'Mixed_6a\'):\n+                with tf.compat.v1.variable_scope(\'Mixed_6a\'):\n                     net = reduction_a(net, 192, 192, 256, 384)\n                 end_points[\'Mixed_6a\'] = net\n                 \n@@ -217,7 +217,7 @@ def inception_resnet_v1(inputs, is_training=True,\n                 end_points[\'Mixed_6b\'] = net\n                 \n                 # Reduction-B\n-                with tf.variable_scope(\'Mixed_7a\'):\n+                with tf.compat.v1.variable_scope(\'Mixed_7a\'):\n                     net = reduction_b(net)\n                 end_points[\'Mixed_7a\'] = net\n                 \n@@ -228,7 +228,7 @@ def inception_resnet_v1(inputs, is_training=True,\n                 net = block8(net, activation_fn=None)\n                 end_points[\'Mixed_8b\'] = net\n                 \n-                with tf.variable_scope(\'Logits\'):\n+                with tf.compat.v1.variable_scope(\'Logits\'):\n                     end_points[\'PrePool\'] = net\n                     #pylint: disable=no-member\n                     net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\ndiff --git a/src/models/inception_resnet_v2.py b/src/models/inception_resnet_v2.py\nindex 0fb176f..534fce1 100644\n--- a/src/models/inception_resnet_v2.py\n+++ b/src/models/inception_resnet_v2.py\n@@ -24,18 +24,18 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n+import tf_slim as slim\n \n # Inception-Resnet-A\n def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n     """Builds the 35x35 resnet block."""\n-    with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n+    with tf.compat.v1.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n+        with tf.compat.v1.variable_scope(\'Branch_0\'):\n             tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n+        with tf.compat.v1.variable_scope(\'Branch_1\'):\n             tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n             tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n-        with tf.variable_scope(\'Branch_2\'):\n+        with tf.compat.v1.variable_scope(\'Branch_2\'):\n             tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n             tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n             tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n@@ -50,10 +50,10 @@ def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n # Inception-Resnet-B\n def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n     """Builds the 17x17 resnet block."""\n-    with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n+    with tf.compat.v1.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n+        with tf.compat.v1.variable_scope(\'Branch_0\'):\n             tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n+        with tf.compat.v1.variable_scope(\'Branch_1\'):\n             tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n             tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                         scope=\'Conv2d_0b_1x7\')\n@@ -71,10 +71,10 @@ def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n # Inception-Resnet-C\n def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n     """Builds the 8x8 resnet block."""\n-    with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n+    with tf.compat.v1.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n+        with tf.compat.v1.variable_scope(\'Branch_0\'):\n             tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n+        with tf.compat.v1.variable_scope(\'Branch_1\'):\n             tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n             tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                         scope=\'Conv2d_0b_1x3\')\n@@ -98,11 +98,11 @@ def inference(images, keep_probability, phase_train=True,\n         # force in-place updates of mean and variance estimates\n         \'updates_collections\': None,\n         # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n+        \'variables_collections\': [ tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES ],\n }\n     with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                         weights_initializer=slim.initializers.xavier_initializer(), \n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n+                        weights_regularizer=tf.keras.regularizers.l2(0.5 * (weight_decay)),\n                         normalizer_fn=slim.batch_norm,\n                         normalizer_params=batch_norm_params):\n         return inception_resnet_v2(images, is_training=phase_train,\n@@ -129,7 +129,7 @@ def inception_resnet_v2(inputs, is_training=True,\n     """\n     end_points = {}\n   \n-    with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs], reuse=reuse):\n+    with tf.compat.v1.variable_scope(scope, \'InceptionResnetV2\', [inputs], reuse=reuse):\n         with slim.arg_scope([slim.batch_norm, slim.dropout],\n                             is_training=is_training):\n             with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n@@ -164,20 +164,20 @@ def inception_resnet_v2(inputs, is_training=True,\n                 end_points[\'MaxPool_5a_3x3\'] = net\n         \n                 # 35 x 35 x 320\n-                with tf.variable_scope(\'Mixed_5b\'):\n-                    with tf.variable_scope(\'Branch_0\'):\n+                with tf.compat.v1.variable_scope(\'Mixed_5b\'):\n+                    with tf.compat.v1.variable_scope(\'Branch_0\'):\n                         tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n-                    with tf.variable_scope(\'Branch_1\'):\n+                    with tf.compat.v1.variable_scope(\'Branch_1\'):\n                         tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n                         tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                                     scope=\'Conv2d_0b_5x5\')\n-                    with tf.variable_scope(\'Branch_2\'):\n+                    with tf.compat.v1.variable_scope(\'Branch_2\'):\n                         tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n                         tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                                     scope=\'Conv2d_0b_3x3\')\n                         tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                                     scope=\'Conv2d_0c_3x3\')\n-                    with tf.variable_scope(\'Branch_3\'):\n+                    with tf.compat.v1.variable_scope(\'Branch_3\'):\n                         tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n                                                      scope=\'AvgPool_0a_3x3\')\n                         tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n@@ -189,18 +189,18 @@ def inception_resnet_v2(inputs, is_training=True,\n                 net = slim.repeat(net, 10, block35, scale=0.17)\n         \n                 # 17 x 17 x 1024\n-                with tf.variable_scope(\'Mixed_6a\'):\n-                    with tf.variable_scope(\'Branch_0\'):\n+                with tf.compat.v1.variable_scope(\'Mixed_6a\'):\n+                    with tf.compat.v1.variable_scope(\'Branch_0\'):\n                         tower_conv = slim.conv2d(net, 384, 3, stride=2, padding=\'VALID\',\n                                                  scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_1\'):\n+                    with tf.compat.v1.variable_scope(\'Branch_1\'):\n                         tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n                         tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                                     scope=\'Conv2d_0b_3x3\')\n                         tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                                     stride=2, padding=\'VALID\',\n                                                     scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_2\'):\n+                    with tf.compat.v1.variable_scope(\'Branch_2\'):\n                         tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                                      scope=\'MaxPool_1a_3x3\')\n                     net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n@@ -208,22 +208,22 @@ def inception_resnet_v2(inputs, is_training=True,\n                 end_points[\'Mixed_6a\'] = net\n                 net = slim.repeat(net, 20, block17, scale=0.10)\n         \n-                with tf.variable_scope(\'Mixed_7a\'):\n-                    with tf.variable_scope(\'Branch_0\'):\n+                with tf.compat.v1.variable_scope(\'Mixed_7a\'):\n+                    with tf.compat.v1.variable_scope(\'Branch_0\'):\n                         tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n                         tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_1\'):\n+                    with tf.compat.v1.variable_scope(\'Branch_1\'):\n                         tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n                         tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                                     padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_2\'):\n+                    with tf.compat.v1.variable_scope(\'Branch_2\'):\n                         tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n                         tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                                     scope=\'Conv2d_0b_3x3\')\n                         tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                                     padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_3\'):\n+                    with tf.compat.v1.variable_scope(\'Branch_3\'):\n                         tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                                      scope=\'MaxPool_1a_3x3\')\n                     net = tf.concat([tower_conv_1, tower_conv1_1,\n@@ -237,7 +237,7 @@ def inception_resnet_v2(inputs, is_training=True,\n                 net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n                 end_points[\'Conv2d_7b_1x1\'] = net\n         \n-                with tf.variable_scope(\'Logits\'):\n+                with tf.compat.v1.variable_scope(\'Logits\'):\n                     end_points[\'PrePool\'] = net\n                     #pylint: disable=no-member\n                     net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\ndiff --git a/src/models/squeezenet.py b/src/models/squeezenet.py\nindex ae117e1..f88b94d 100644\n--- a/src/models/squeezenet.py\n+++ b/src/models/squeezenet.py\n@@ -3,7 +3,7 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n+import tf_slim as slim\n \n def fire_module(inputs,\n                 squeeze_depth,\n@@ -11,7 +11,7 @@ def fire_module(inputs,\n                 reuse=None,\n                 scope=None,\n                 outputs_collections=None):\n-    with tf.variable_scope(scope, \'fire\', [inputs], reuse=reuse):\n+    with tf.compat.v1.variable_scope(scope, \'fire\', [inputs], reuse=reuse):\n         with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                             outputs_collections=None):\n             net = squeeze(inputs, squeeze_depth)\n@@ -22,7 +22,7 @@ def squeeze(inputs, num_outputs):\n     return slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope=\'squeeze\')\n \n def expand(inputs, num_outputs):\n-    with tf.variable_scope(\'expand\'):\n+    with tf.compat.v1.variable_scope(\'expand\'):\n         e1x1 = slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope=\'1x1\')\n         e3x3 = slim.conv2d(inputs, num_outputs, [3, 3], scope=\'3x3\')\n     return tf.concat([e1x1, e3x3], 3)\n@@ -36,14 +36,14 @@ def inference(images, keep_probability, phase_train=True, bottleneck_layer_size=\n         # force in-place updates of mean and variance estimates\n         \'updates_collections\': None,\n         # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n+        \'variables_collections\': [ tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES ],\n     }\n     with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                        weights_initializer=slim.xavier_initializer_conv2d(uniform=True),\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n+                        weights_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode="fan_avg", distribution=("uniform" if True else "truncated_normal")),\n+                        weights_regularizer=tf.keras.regularizers.l2(0.5 * (weight_decay)),\n                         normalizer_fn=slim.batch_norm,\n                         normalizer_params=batch_norm_params):\n-        with tf.variable_scope(\'squeezenet\', [images], reuse=reuse):\n+        with tf.compat.v1.variable_scope(\'squeezenet\', [images], reuse=reuse):\n             with slim.arg_scope([slim.batch_norm, slim.dropout],\n                                 is_training=phase_train):\n                 net = slim.conv2d(images, 96, [7, 7], stride=2, scope=\'conv1\')\ndiff --git a/src/train_softmax.py b/src/train_softmax.py\nindex 6b0b28b..5ee0f96 100644\n--- a/src/train_softmax.py\n+++ b/src/train_softmax.py\n@@ -39,7 +39,7 @@ import facenet\n import lfw\n import h5py\n import math\n-import tensorflow.contrib.slim as slim\n+import tf_slim as slim\n from tensorflow.python.ops import data_flow_ops\n from tensorflow.python.framework import ops\n from tensorflow.python.ops import array_ops\n@@ -95,7 +95,7 @@ def main(args):\n         lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n     \n     with tf.Graph().as_default():\n-        tf.set_random_seed(args.seed)\n+        tf.compat.v1.set_random_seed(args.seed)\n         global_step = tf.Variable(0, trainable=False)\n         \n         # Get a list of image paths and their labels\n@@ -107,17 +107,17 @@ def main(args):\n         # Create a queue that produces indices into the image_list and label_list \n         labels = ops.convert_to_tensor(label_list, dtype=tf.int32)\n         range_size = array_ops.shape(labels)[0]\n-        index_queue = tf.train.range_input_producer(range_size, num_epochs=None,\n+        index_queue = tf.compat.v1.train.range_input_producer(range_size, num_epochs=None,\n                              shuffle=True, seed=None, capacity=32)\n         \n         index_dequeue_op = index_queue.dequeue_many(args.batch_size*args.epoch_size, \'index_dequeue\')\n         \n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\'learning_rate\')\n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\'image_paths\')\n-        labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'labels\')\n-        control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'control\')\n+        learning_rate_placeholder = tf.compat.v1.placeholder(tf.float32, name=\'learning_rate\')\n+        batch_size_placeholder = tf.compat.v1.placeholder(tf.int32, name=\'batch_size\')\n+        phase_train_placeholder = tf.compat.v1.placeholder(tf.bool, name=\'phase_train\')\n+        image_paths_placeholder = tf.compat.v1.placeholder(tf.string, shape=(None,1), name=\'image_paths\')\n+        labels_placeholder = tf.compat.v1.placeholder(tf.int32, shape=(None,1), name=\'labels\')\n+        control_placeholder = tf.compat.v1.placeholder(tf.int32, shape=(None,1), name=\'control\')\n         \n         nrof_preprocess_threads = 4\n         input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\n@@ -145,55 +145,55 @@ def main(args):\n             weight_decay=args.weight_decay)\n         logits = slim.fully_connected(prelogits, len(train_set), activation_fn=None, \n                 weights_initializer=slim.initializers.xavier_initializer(), \n-                weights_regularizer=slim.l2_regularizer(args.weight_decay),\n+                weights_regularizer=tf.keras.regularizers.l2(0.5 * (args.weight_decay)),\n                 scope=\'Logits\', reuse=False)\n \n         embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\'embeddings\')\n \n         # Norm for the prelogits\n         eps = 1e-4\n-        prelogits_norm = tf.reduce_mean(tf.norm(tf.abs(prelogits)+eps, ord=args.prelogits_norm_p, axis=1))\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_norm * args.prelogits_norm_loss_factor)\n+        prelogits_norm = tf.reduce_mean(input_tensor=tf.norm(tensor=tf.abs(prelogits)+eps, ord=args.prelogits_norm_p, axis=1))\n+        tf.compat.v1.add_to_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES, prelogits_norm * args.prelogits_norm_loss_factor)\n \n         # Add center loss\n         prelogits_center_loss, _ = facenet.center_loss(prelogits, label_batch, args.center_loss_alfa, nrof_classes)\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_center_loss * args.center_loss_factor)\n+        tf.compat.v1.add_to_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES, prelogits_center_loss * args.center_loss_factor)\n \n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\n+        learning_rate = tf.compat.v1.train.exponential_decay(learning_rate_placeholder, global_step,\n             args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\n-        tf.summary.scalar(\'learning_rate\', learning_rate)\n+        tf.compat.v1.summary.scalar(\'learning_rate\', learning_rate)\n \n         # Calculate the average cross entropy loss across the batch\n         cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n             labels=label_batch, logits=logits, name=\'cross_entropy_per_example\')\n-        cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\'cross_entropy\')\n-        tf.add_to_collection(\'losses\', cross_entropy_mean)\n+        cross_entropy_mean = tf.reduce_mean(input_tensor=cross_entropy, name=\'cross_entropy\')\n+        tf.compat.v1.add_to_collection(\'losses\', cross_entropy_mean)\n         \n-        correct_prediction = tf.cast(tf.equal(tf.argmax(logits, 1), tf.cast(label_batch, tf.int64)), tf.float32)\n-        accuracy = tf.reduce_mean(correct_prediction)\n+        correct_prediction = tf.cast(tf.equal(tf.argmax(input=logits, axis=1), tf.cast(label_batch, tf.int64)), tf.float32)\n+        accuracy = tf.reduce_mean(input_tensor=correct_prediction)\n         \n         # Calculate the total losses\n-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n+        regularization_losses = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n         total_loss = tf.add_n([cross_entropy_mean] + regularization_losses, name=\'total_loss\')\n \n         # Build a Graph that trains the model with one batch of examples and updates the model parameters\n         train_op = facenet.train(total_loss, global_step, args.optimizer, \n-            learning_rate, args.moving_average_decay, tf.global_variables(), args.log_histograms)\n+            learning_rate, args.moving_average_decay, tf.compat.v1.global_variables(), args.log_histograms)\n         \n         # Create a saver\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n+        saver = tf.compat.v1.train.Saver(tf.compat.v1.trainable_variables(), max_to_keep=3)\n \n         # Build the summary operation based on the TF collection of Summaries.\n-        summary_op = tf.summary.merge_all()\n+        summary_op = tf.compat.v1.summary.merge_all()\n \n         # Start running operations on the Graph.\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-        sess.run(tf.global_variables_initializer())\n-        sess.run(tf.local_variables_initializer())\n-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n+        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n+        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        sess.run(tf.compat.v1.global_variables_initializer())\n+        sess.run(tf.compat.v1.local_variables_initializer())\n+        summary_writer = tf.compat.v1.summary.FileWriter(log_dir, sess.graph)\n         coord = tf.train.Coordinator()\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\n+        tf.compat.v1.train.start_queue_runners(coord=coord, sess=sess)\n \n         with sess.as_default():\n \n@@ -347,7 +347,7 @@ def train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_o\n         batch_number += 1\n         train_time += duration\n     # Add validation loss and accuracy to summary\n-    summary = tf.Summary()\n+    summary = tf.compat.v1.Summary()\n     #pylint: disable=maybe-no-member\n     summary.value.add(tag=\'time/total\', simple_value=train_time)\n     summary_writer.add_summary(summary, global_step=step_)\n@@ -443,7 +443,7 @@ def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phas\n     print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n     lfw_time = time.time() - start_time\n     # Add validation loss and accuracy to summary\n-    summary = tf.Summary()\n+    summary = tf.compat.v1.Summary()\n     #pylint: disable=maybe-no-member\n     summary.value.add(tag=\'lfw/accuracy\', simple_value=np.mean(accuracy))\n     summary.value.add(tag=\'lfw/val_rate\', simple_value=val)\n@@ -470,7 +470,7 @@ def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_n\n         saver.export_meta_graph(metagraph_filename)\n         save_time_metagraph = time.time() - start_time\n         print(\'Metagraph saved in %.2f seconds\' % save_time_metagraph)\n-    summary = tf.Summary()\n+    summary = tf.compat.v1.Summary()\n     #pylint: disable=maybe-no-member\n     summary.value.add(tag=\'time/save_variables\', simple_value=save_time_variables)\n     summary.value.add(tag=\'time/save_metagraph\', simple_value=save_time_metagraph)\ndiff --git a/src/train_tripletloss.py b/src/train_tripletloss.py\nindex d6df19a..4c0fe89 100644\n--- a/src/train_tripletloss.py\n+++ b/src/train_tripletloss.py\n@@ -79,18 +79,18 @@ def main(args):\n         \n     \n     with tf.Graph().as_default():\n-        tf.set_random_seed(args.seed)\n+        tf.compat.v1.set_random_seed(args.seed)\n         global_step = tf.Variable(0, trainable=False)\n \n         # Placeholder for the learning rate\n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\'learning_rate\')\n+        learning_rate_placeholder = tf.compat.v1.placeholder(tf.float32, name=\'learning_rate\')\n         \n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n+        batch_size_placeholder = tf.compat.v1.placeholder(tf.int32, name=\'batch_size\')\n         \n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n+        phase_train_placeholder = tf.compat.v1.placeholder(tf.bool, name=\'phase_train\')\n         \n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,3), name=\'image_paths\')\n-        labels_placeholder = tf.placeholder(tf.int64, shape=(None,3), name=\'labels\')\n+        image_paths_placeholder = tf.compat.v1.placeholder(tf.string, shape=(None,3), name=\'image_paths\')\n+        labels_placeholder = tf.compat.v1.placeholder(tf.int64, shape=(None,3), name=\'labels\')\n         \n         input_queue = data_flow_ops.FIFOQueue(capacity=100000,\n                                     dtypes=[tf.string, tf.int64],\n@@ -104,13 +104,13 @@ def main(args):\n             filenames, label = input_queue.dequeue()\n             images = []\n             for filename in tf.unstack(filenames):\n-                file_contents = tf.read_file(filename)\n+                file_contents = tf.io.read_file(filename)\n                 image = tf.image.decode_image(file_contents, channels=3)\n                 \n                 if args.random_crop:\n-                    image = tf.random_crop(image, [args.image_size, args.image_size, 3])\n+                    image = tf.image.random_crop(image, [args.image_size, args.image_size, 3])\n                 else:\n-                    image = tf.image.resize_image_with_crop_or_pad(image, args.image_size, args.image_size)\n+                    image = tf.image.resize_with_crop_or_pad(image, args.image_size, args.image_size)\n                 if args.random_flip:\n                     image = tf.image.random_flip_left_right(image)\n     \n@@ -119,7 +119,7 @@ def main(args):\n                 images.append(tf.image.per_image_standardization(image))\n             images_and_labels.append([images, label])\n     \n-        image_batch, labels_batch = tf.train.batch_join(\n+        image_batch, labels_batch = tf.compat.v1.train.batch_join(\n             images_and_labels, batch_size=batch_size_placeholder, \n             shapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True,\n             capacity=4 * nrof_preprocess_threads * args.batch_size,\n@@ -138,35 +138,35 @@ def main(args):\n         anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\n         triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\n         \n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\n+        learning_rate = tf.compat.v1.train.exponential_decay(learning_rate_placeholder, global_step,\n             args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\n-        tf.summary.scalar(\'learning_rate\', learning_rate)\n+        tf.compat.v1.summary.scalar(\'learning_rate\', learning_rate)\n \n         # Calculate the total losses\n-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n+        regularization_losses = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n         total_loss = tf.add_n([triplet_loss] + regularization_losses, name=\'total_loss\')\n \n         # Build a Graph that trains the model with one batch of examples and updates the model parameters\n         train_op = facenet.train(total_loss, global_step, args.optimizer, \n-            learning_rate, args.moving_average_decay, tf.global_variables())\n+            learning_rate, args.moving_average_decay, tf.compat.v1.global_variables())\n         \n         # Create a saver\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n+        saver = tf.compat.v1.train.Saver(tf.compat.v1.trainable_variables(), max_to_keep=3)\n \n         # Build the summary operation based on the TF collection of Summaries.\n-        summary_op = tf.summary.merge_all()\n+        summary_op = tf.compat.v1.summary.merge_all()\n \n         # Start running operations on the Graph.\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        \n+        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n+        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))        \n \n         # Initialize variables\n-        sess.run(tf.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\n-        sess.run(tf.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\n+        sess.run(tf.compat.v1.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\n+        sess.run(tf.compat.v1.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\n \n-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n+        summary_writer = tf.compat.v1.summary.FileWriter(log_dir, sess.graph)\n         coord = tf.train.Coordinator()\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\n+        tf.compat.v1.train.start_queue_runners(coord=coord, sess=sess)\n \n         with sess.as_default():\n \n@@ -245,7 +245,7 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\n         i = 0\n         emb_array = np.zeros((nrof_examples, embedding_size))\n         loss_array = np.zeros((nrof_triplets,))\n-        summary = tf.Summary()\n+        summary = tf.compat.v1.Summary()\n         step = 0\n         while i < nrof_batches:\n             start_time = time.time()\n@@ -369,7 +369,7 @@ def evaluate(sess, image_paths, embeddings, labels_batch, image_paths_placeholde\n     print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n     lfw_time = time.time() - start_time\n     # Add validation loss and accuracy to summary\n-    summary = tf.Summary()\n+    summary = tf.compat.v1.Summary()\n     #pylint: disable=maybe-no-member\n     summary.value.add(tag=\'lfw/accuracy\', simple_value=np.mean(accuracy))\n     summary.value.add(tag=\'lfw/val_rate\', simple_value=val)\n@@ -394,7 +394,7 @@ def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_n\n         saver.export_meta_graph(metagraph_filename)\n         save_time_metagraph = time.time() - start_time\n         print(\'Metagraph saved in %.2f seconds\' % save_time_metagraph)\n-    summary = tf.Summary()\n+    summary = tf.compat.v1.Summary()\n     #pylint: disable=maybe-no-member\n     summary.value.add(tag=\'time/save_variables\', simple_value=save_time_variables)\n     summary.value.add(tag=\'time/save_metagraph\', simple_value=save_time_metagraph)\n@@ -419,16 +419,16 @@ def parse_arguments(argv):\n     parser = argparse.ArgumentParser()\n     \n     parser.add_argument(\'--logs_base_dir\', type=str, \n-        help=\'Directory where to write event logs.\', default=\'~/logs/facenet\')\n+        help=\'Directory where to write event logs.\', default=\'./logs/facenet\')\n     parser.add_argument(\'--models_base_dir\', type=str,\n-        help=\'Directory where to write trained models and checkpoints.\', default=\'~/models/facenet\')\n+        help=\'Directory where to write trained models and checkpoints.\', default=\'./models/facenet\')\n     parser.add_argument(\'--gpu_memory_fraction\', type=float,\n         help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n     parser.add_argument(\'--pretrained_model\', type=str,\n         help=\'Load a pretrained model before training starts.\')\n     parser.add_argument(\'--data_dir\', type=str,\n         help=\'Path to the data directory containing aligned face patches.\',\n-        default=\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\')\n+        default=\'./datasets/casia/casia_maxpy_mtcnnalign_182_160\')\n     parser.add_argument(\'--model_def\', type=str,\n         help=\'Model definition. Points to a module containing the definition of the inference graph.\', default=\'models.inception_resnet_v1\')\n     parser.add_argument(\'--max_nrof_epochs\', type=int,\ndiff --git a/src/validate_on_lfw.py b/src/validate_on_lfw.py\nindex ac456c5..f3eea44 100644\n--- a/src/validate_on_lfw.py\n+++ b/src/validate_on_lfw.py\n@@ -45,7 +45,7 @@ def main(args):\n   \n     with tf.Graph().as_default():\n       \n-        with tf.Session() as sess:\n+        with tf.compat.v1.Session() as sess:\n             \n             # Read the file containing the pairs used for testing\n             pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n@@ -53,11 +53,11 @@ def main(args):\n             # Get the paths for the corresponding images\n             paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n             \n-            image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\'image_paths\')\n-            labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'labels\')\n-            batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n-            control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'control\')\n-            phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n+            image_paths_placeholder = tf.compat.v1.placeholder(tf.string, shape=(None,1), name=\'image_paths\')\n+            labels_placeholder = tf.compat.v1.placeholder(tf.int32, shape=(None,1), name=\'labels\')\n+            batch_size_placeholder = tf.compat.v1.placeholder(tf.int32, name=\'batch_size\')\n+            control_placeholder = tf.compat.v1.placeholder(tf.int32, shape=(None,1), name=\'control\')\n+            phase_train_placeholder = tf.compat.v1.placeholder(tf.bool, name=\'phase_train\')\n  \n             nrof_preprocess_threads = 4\n             image_size = (args.image_size, args.image_size)\n@@ -73,10 +73,10 @@ def main(args):\n             facenet.load_model(args.model, input_map=input_map)\n \n             # Get output tensor\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n #              \n             coord = tf.train.Coordinator()\n-            tf.train.start_queue_runners(coord=coord, sess=sess)\n+            tf.compat.v1.train.start_queue_runners(coord=coord, sess=sess)\n \n             evaluate(sess, eval_enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\n                 embeddings, label_batch, paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, args.distance_metric, args.subtract_mean,\ndiff --git a/test/batch_norm_test.py b/test/batch_norm_test.py\nindex 48cfd55..56eee00 100644\n--- a/test/batch_norm_test.py\n+++ b/test/batch_norm_test.py\n@@ -32,18 +32,18 @@ class BatchNormTest(unittest.TestCase):\n     @unittest.skip("Skip batch norm test case")\n     def testBatchNorm(self):\n       \n-        tf.set_random_seed(123)\n+        tf.compat.v1.set_random_seed(123)\n   \n-        x = tf.placeholder(tf.float32, [None, 20, 20, 10], name=\'input\')\n-        phase_train = tf.placeholder(tf.bool, name=\'phase_train\')\n+        x = tf.compat.v1.placeholder(tf.float32, [None, 20, 20, 10], name=\'input\')\n+        phase_train = tf.compat.v1.placeholder(tf.bool, name=\'phase_train\')\n         \n         # generate random noise to pass into batch norm\n         #x_gen = tf.random_normal([50,20,20,10])\n         \n         bn = models.network.batch_norm(x, phase_train)\n         \n-        init = tf.global_variables_initializer()\n-        sess = tf.Session(config=tf.ConfigProto())\n+        init = tf.compat.v1.global_variables_initializer()\n+        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto())\n         sess.run(init)\n   \n         with sess.as_default():\n@@ -63,4 +63,3 @@ class BatchNormTest(unittest.TestCase):\n \n if __name__ == "__main__":\n     unittest.main()\n-    \n\\ No newline at end of file\ndiff --git a/test/center_loss_test.py b/test/center_loss_test.py\nindex 196cd11..baffd7b 100644\n--- a/test/center_loss_test.py\n+++ b/test/center_loss_test.py\n@@ -37,8 +37,8 @@ class CenterLossTest(unittest.TestCase):\n         \n         with tf.Graph().as_default():\n         \n-            features = tf.placeholder(tf.float32, shape=(batch_size, nrof_features), name=\'features\')\n-            labels = tf.placeholder(tf.int32, shape=(batch_size,), name=\'labels\')\n+            features = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, nrof_features), name=\'features\')\n+            labels = tf.compat.v1.placeholder(tf.int32, shape=(batch_size,), name=\'labels\')\n \n             # Define center loss\n             center_loss, centers = facenet.center_loss(features, labels, alfa, nrof_classes)\n@@ -50,9 +50,9 @@ class CenterLossTest(unittest.TestCase):\n                  [ 3,-3],  [ 3,-1],  [ 3,1],  [ 3,3] \n                  ])\n                 \n-            sess = tf.Session()\n+            sess = tf.compat.v1.Session()\n             with sess.as_default():\n-                sess.run(tf.global_variables_initializer())\n+                sess.run(tf.compat.v1.global_variables_initializer())\n                 np.random.seed(seed=666)\n                 \n                 for _ in range(0,100):\ndiff --git a/test/restore_test.py b/test/restore_test.py\nindex befb04d..7081fde 100644\n--- a/test/restore_test.py\n+++ b/test/restore_test.py\n@@ -47,22 +47,22 @@ class TrainTest(unittest.TestCase):\n         # Try to find values for W and b that compute y_data = W * x_data + b\n         # (We know that W should be 0.1 and b 0.3, but TensorFlow will\n         # figure that out for us.)\n-        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\'W\')\n+        W = tf.Variable(tf.random.uniform([1], -1.0, 1.0), name=\'W\')\n         b = tf.Variable(tf.zeros([1]), name=\'b\')\n         y = W * x_data + b\n         \n         # Minimize the mean squared errors.\n-        loss = tf.reduce_mean(tf.square(y - y_data))\n-        optimizer = tf.train.GradientDescentOptimizer(0.5)\n+        loss = tf.reduce_mean(input_tensor=tf.square(y - y_data))\n+        optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.5)\n         train = optimizer.minimize(loss)\n         \n         # Before starting, initialize the variables.  We will \'run\' this first.\n-        init = tf.global_variables_initializer()\n+        init = tf.compat.v1.global_variables_initializer()\n \n-        saver = tf.train.Saver(tf.trainable_variables())\n+        saver = tf.compat.v1.train.Saver(tf.compat.v1.trainable_variables())\n         \n         # Launch the graph.\n-        sess = tf.Session()\n+        sess = tf.compat.v1.Session()\n         sess.run(init)\n         \n         # Fit the line.\n@@ -74,10 +74,10 @@ class TrainTest(unittest.TestCase):\n         \n         saver.save(sess, os.path.join(self.tmp_dir, "model_ex1"))\n         \n-        tf.reset_default_graph()\n+        tf.compat.v1.reset_default_graph()\n \n-        saver = tf.train.import_meta_graph(os.path.join(self.tmp_dir, "model_ex1.meta"))\n-        sess = tf.Session()\n+        saver = tf.compat.v1.train.import_meta_graph(os.path.join(self.tmp_dir, "model_ex1.meta"))\n+        sess = tf.compat.v1.Session()\n         saver.restore(sess, os.path.join(self.tmp_dir, "model_ex1"))\n         \n         w_restored = sess.run(\'W:0\')\n@@ -97,28 +97,28 @@ class TrainTest(unittest.TestCase):\n         # Try to find values for W and b that compute y_data = W * x_data + b\n         # (We know that W should be 0.1 and b 0.3, but TensorFlow will\n         # figure that out for us.)\n-        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\'W\')\n+        W = tf.Variable(tf.random.uniform([1], -1.0, 1.0), name=\'W\')\n         b = tf.Variable(tf.zeros([1]), name=\'b\')\n         y = W * x_data + b\n         \n         # Minimize the mean squared errors.\n-        loss = tf.reduce_mean(tf.square(y - y_data))\n-        optimizer = tf.train.GradientDescentOptimizer(0.5)\n+        loss = tf.reduce_mean(input_tensor=tf.square(y - y_data))\n+        optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.5)\n         opt_op = optimizer.minimize(loss)\n \n         # Track the moving averages of all trainable variables.\n         ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n-        averages_op = ema.apply(tf.trainable_variables())\n+        averages_op = ema.apply(tf.compat.v1.trainable_variables())\n         with tf.control_dependencies([opt_op]):\n             train_op = tf.group(averages_op)\n   \n         # Before starting, initialize the variables.  We will \'run\' this first.\n-        init = tf.global_variables_initializer()\n+        init = tf.compat.v1.global_variables_initializer()\n \n-        saver = tf.train.Saver(tf.trainable_variables())\n+        saver = tf.compat.v1.train.Saver(tf.compat.v1.trainable_variables())\n         \n         # Launch the graph.\n-        sess = tf.Session()\n+        sess = tf.compat.v1.Session()\n         sess.run(init)\n         \n         # Fit the line.\n@@ -130,17 +130,17 @@ class TrainTest(unittest.TestCase):\n         \n         saver.save(sess, os.path.join(self.tmp_dir, "model_ex1"))\n                 \n-        tf.reset_default_graph()\n+        tf.compat.v1.reset_default_graph()\n \n-        tf.train.import_meta_graph(os.path.join(self.tmp_dir, "model_ex1.meta"))\n-        sess = tf.Session()\n+        tf.compat.v1.train.import_meta_graph(os.path.join(self.tmp_dir, "model_ex1.meta"))\n+        sess = tf.compat.v1.Session()\n         \n         print(\'------------------------------------------------------\')\n-        for var in tf.global_variables():\n+        for var in tf.compat.v1.global_variables():\n             print(\'all variables: \' + var.op.name)\n-        for var in tf.trainable_variables():\n+        for var in tf.compat.v1.trainable_variables():\n             print(\'normal variable: \' + var.op.name)\n-        for var in tf.moving_average_variables():\n+        for var in tf.compat.v1.moving_average_variables():\n             print(\'ema variable: \' + var.op.name)\n         print(\'------------------------------------------------------\')\n \n@@ -148,16 +148,16 @@ class TrainTest(unittest.TestCase):\n         restore_vars = {}\n         if mode == 0:\n             ema = tf.train.ExponentialMovingAverage(1.0)\n-            for var in tf.trainable_variables():\n+            for var in tf.compat.v1.trainable_variables():\n                 print(\'%s: %s\' % (ema.average_name(var), var.op.name))\n                 restore_vars[ema.average_name(var)] = var\n         elif mode == 1:\n-            for var in tf.trainable_variables():\n+            for var in tf.compat.v1.trainable_variables():\n                 ema_name = var.op.name + \'/ExponentialMovingAverage\'\n                 print(\'%s: %s\' % (ema_name, var.op.name))\n                 restore_vars[ema_name] = var\n             \n-        saver = tf.train.Saver(restore_vars, name=\'ema_restore\')\n+        saver = tf.compat.v1.train.Saver(restore_vars, name=\'ema_restore\')\n         \n         saver.restore(sess, os.path.join(self.tmp_dir, "model_ex1"))\n         \n@@ -178,4 +178,3 @@ def create_checkpoint_file(model_dir, model_file):\n         \n if __name__ == "__main__":\n     unittest.main()\n-    \n\\ No newline at end of file\ndiff --git a/test/train_test.py b/test/train_test.py\nindex 12cd663..20a763b 100644\n--- a/test/train_test.py\n+++ b/test/train_test.py\n@@ -243,4 +243,3 @@ def create_mock_lfw_pairs(tmp_dir):\n \n if __name__ == "__main__":\n     unittest.main()\n-    \n\\ No newline at end of file\ndiff --git a/test/triplet_loss_test.py b/test/triplet_loss_test.py\nindex 2648b30..9e3c781 100644\n--- a/test/triplet_loss_test.py\n+++ b/test/triplet_loss_test.py\n@@ -34,11 +34,11 @@ class DemuxEmbeddingsTest(unittest.TestCase):\n         \n         with tf.Graph().as_default():\n         \n-            embeddings = tf.placeholder(tf.float64, shape=(batch_size, embedding_size), name=\'embeddings\')\n+            embeddings = tf.compat.v1.placeholder(tf.float64, shape=(batch_size, embedding_size), name=\'embeddings\')\n             anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,embedding_size]), 3, 1)\n             triplet_loss = facenet.triplet_loss(anchor, positive, negative, alpha)\n                 \n-            sess = tf.Session()\n+            sess = tf.compat.v1.Session()\n             with sess.as_default():\n                 np.random.seed(seed=666)\n                 emb = np.random.uniform(size=(batch_size, embedding_size))\ndiff --git a/tmp/deepdream.py b/tmp/deepdream.py\nindex 604636b..e920198 100644\n--- a/tmp/deepdream.py\n+++ b/tmp/deepdream.py\n@@ -32,11 +32,11 @@ def main():\n     \n     # creating TensorFlow session and loading the model\n     graph = tf.Graph()\n-    sess = tf.InteractiveSession(graph=graph)\n-    with tf.gfile.FastGFile(os.path.join(data_dir, model_fn), \'rb\') as f:\n-        graph_def = tf.GraphDef()\n+    sess = tf.compat.v1.InteractiveSession(graph=graph)\n+    with tf.compat.v1.gfile.FastGFile(os.path.join(data_dir, model_fn), \'rb\') as f:\n+        graph_def = tf.compat.v1.GraphDef()\n         graph_def.ParseFromString(f.read())\n-    t_input = tf.placeholder(np.float32, name=\'input\') # define the input tensor\n+    t_input = tf.compat.v1.placeholder(np.float32, name=\'input\') # define the input tensor\n     imagenet_mean = 117.0\n     t_preprocessed = tf.expand_dims(t_input-imagenet_mean, 0)\n     tf.import_graph_def(graph_def, {\'input\':t_preprocessed})\n@@ -52,7 +52,7 @@ def main():\n     #pylint: disable=unused-variable\n     def strip_consts(graph_def, max_const_size=32):\n         """Strip large constant values from graph_def."""\n-        strip_def = tf.GraphDef()\n+        strip_def = tf.compat.v1.GraphDef()\n         for n0 in graph_def.node:\n             n = strip_def.node.add() #pylint: disable=maybe-no-member\n             n.MergeFrom(n0)\n@@ -64,7 +64,7 @@ def main():\n         return strip_def\n       \n     def rename_nodes(graph_def, rename_func):\n-        res_def = tf.GraphDef()\n+        res_def = tf.compat.v1.GraphDef()\n         for n0 in graph_def.node:\n             n = res_def.node.add() #pylint: disable=maybe-no-member\n             n.MergeFrom(n0)\n@@ -87,8 +87,8 @@ def main():\n         return graph.get_tensor_by_name("import/%s:0"%layer)\n     \n     def render_naive(t_obj, img0=img_noise, iter_n=20, step=1.0):\n-        t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n-        t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n+        t_score = tf.reduce_mean(input_tensor=t_obj) # defining the optimization objective\n+        t_grad = tf.gradients(ys=t_score, xs=t_input)[0] # behold the power of automatic differentiation!\n         \n         img = img0.copy()\n         for _ in range(iter_n):\n@@ -102,7 +102,7 @@ def main():\n         \'\'\'Helper that transforms TF-graph generating function into a regular one.\n         See "resize" function below.\n         \'\'\'\n-        placeholders = list(map(tf.placeholder, argtypes))\n+        placeholders = list(map(tf.compat.v1.placeholder, argtypes))\n         def wrap(f):\n             out = f(*placeholders)\n             def wrapper(*args, **kw):\n@@ -113,7 +113,7 @@ def main():\n     # Helper function that uses TF to resize an image\n     def resize(img, size):\n         img = tf.expand_dims(img, 0)\n-        return tf.image.resize_bilinear(img, size)[0,:,:,:]\n+        return tf.image.resize(img, size, method=tf.image.ResizeMethod.BILINEAR)[0,:,:,:]\n     resize = tffunc(np.float32, np.int32)(resize)\n     \n     \n@@ -134,8 +134,8 @@ def main():\n         return np.roll(np.roll(grad, -sx, 1), -sy, 0)    \n       \n     def render_multiscale(t_obj, img0=img_noise, iter_n=10, step=1.0, octave_n=3, octave_scale=1.4):\n-        t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n-        t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n+        t_score = tf.reduce_mean(input_tensor=t_obj) # defining the optimization objective\n+        t_grad = tf.gradients(ys=t_score, xs=t_input)[0] # behold the power of automatic differentiation!\n         \n         img = img0.copy()\n         for octave in range(octave_n):\n@@ -151,9 +151,9 @@ def main():\n             \n     def lap_split(img):\n         \'\'\'Split the image into lo and hi frequency components\'\'\'\n-        with tf.name_scope(\'split\'):\n-            lo = tf.nn.conv2d(img, k5x5, [1,2,2,1], \'SAME\')\n-            lo2 = tf.nn.conv2d_transpose(lo, k5x5*4, tf.shape(img), [1,2,2,1])\n+        with tf.compat.v1.name_scope(\'split\'):\n+            lo = tf.nn.conv2d(input=img, filters=k5x5, strides=[1,2,2,1], padding=\'SAME\')\n+            lo2 = tf.nn.conv2d_transpose(lo, k5x5*4, tf.shape(input=img), [1,2,2,1])\n             hi = img-lo2\n         return lo, hi\n     \n@@ -170,14 +170,14 @@ def main():\n         \'\'\'Merge Laplacian pyramid\'\'\'\n         img = levels[0]\n         for hi in levels[1:]:\n-            with tf.name_scope(\'merge\'):\n-                img = tf.nn.conv2d_transpose(img, k5x5*4, tf.shape(hi), [1,2,2,1]) + hi\n+            with tf.compat.v1.name_scope(\'merge\'):\n+                img = tf.nn.conv2d_transpose(img, k5x5*4, tf.shape(input=hi), [1,2,2,1]) + hi\n         return img\n     \n     def normalize_std(img, eps=1e-10):\n         \'\'\'Normalize image by making its standard deviation = 1.0\'\'\'\n-        with tf.name_scope(\'normalize\'):\n-            std = tf.sqrt(tf.reduce_mean(tf.square(img)))\n+        with tf.compat.v1.name_scope(\'normalize\'):\n+            std = tf.sqrt(tf.reduce_mean(input_tensor=tf.square(img)))\n             return img/tf.maximum(std, eps)\n     \n     def lap_normalize(img, scale_n=4):\n@@ -190,8 +190,8 @@ def main():\n   \n     def render_lapnorm(t_obj, img0=img_noise, visfunc=visstd,\n                        iter_n=10, step=1.0, octave_n=3, octave_scale=1.4, lap_n=4):\n-        t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n-        t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n+        t_score = tf.reduce_mean(input_tensor=t_obj) # defining the optimization objective\n+        t_grad = tf.gradients(ys=t_score, xs=t_input)[0] # behold the power of automatic differentiation!\n         # build the laplacian normalization graph\n         lap_norm_func = tffunc(np.float32)(partial(lap_normalize, scale_n=lap_n))\n     \n@@ -208,8 +208,8 @@ def main():\n   \n     def render_deepdream(t_obj, img0=img_noise,\n                          iter_n=10, step=1.5, octave_n=4, octave_scale=1.4):\n-        t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n-        t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n+        t_score = tf.reduce_mean(input_tensor=t_obj) # defining the optimization objective\n+        t_grad = tf.gradients(ys=t_score, xs=t_input)[0] # behold the power of automatic differentiation!\n     \n         # split the image into a number of octaves\n         img = img0\ndiff --git a/tmp/mnist_center_loss.py b/tmp/mnist_center_loss.py\nindex 1122f7a..8048900 100644\n--- a/tmp/mnist_center_loss.py\n+++ b/tmp/mnist_center_loss.py\n@@ -49,10 +49,10 @@ EVAL_BATCH_SIZE = 64\n EVAL_FREQUENCY = 100  # Number of steps between evaluations.\n \n \n-tf.app.flags.DEFINE_boolean("self_test", False, "True if running a self test.")\n-tf.app.flags.DEFINE_boolean(\'use_fp16\', False,\n+tf.compat.v1.app.flags.DEFINE_boolean("self_test", False, "True if running a self test.")\n+tf.compat.v1.app.flags.DEFINE_boolean(\'use_fp16\', False,\n                             "Use half floats instead of full floats if True.")\n-FLAGS = tf.app.flags.FLAGS\n+FLAGS = tf.compat.v1.app.flags.FLAGS\n \n \n def data_type():\n@@ -65,12 +65,12 @@ def data_type():\n \n def maybe_download(filename):\n     """Download the data from Yann\'s website, unless it\'s already here."""\n-    if not tf.gfile.Exists(WORK_DIRECTORY):\n-        tf.gfile.MakeDirs(WORK_DIRECTORY)\n+    if not tf.io.gfile.exists(WORK_DIRECTORY):\n+        tf.io.gfile.makedirs(WORK_DIRECTORY)\n     filepath = os.path.join(WORK_DIRECTORY, filename)\n-    if not tf.gfile.Exists(filepath):\n+    if not tf.io.gfile.exists(filepath):\n         filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n-        with tf.gfile.GFile(filepath) as f:\n+        with tf.io.gfile.GFile(filepath) as f:\n             size = f.size()\n         print(\'Successfully downloaded\', filename, size, \'bytes.\')\n     return filepath\n@@ -152,11 +152,11 @@ def main(argv=None):  # pylint: disable=unused-argument\n     # This is where training samples and labels are fed to the graph.\n     # These placeholder nodes will be fed a batch of training data at each\n     # training step using the {feed_dict} argument to the Run() call below.\n-    train_data_node = tf.placeholder(\n+    train_data_node = tf.compat.v1.placeholder(\n         data_type(),\n         shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n-    train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n-    eval_data = tf.placeholder(\n+    train_labels_node = tf.compat.v1.placeholder(tf.int64, shape=(BATCH_SIZE,))\n+    eval_data = tf.compat.v1.placeholder(\n         data_type(),\n         shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n \n@@ -164,27 +164,27 @@ def main(argv=None):  # pylint: disable=unused-argument\n     # initial value which will be assigned when we call:\n     # {tf.global_variables_initializer().run()}\n     conv1_weights = tf.Variable(\n-        tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n+        tf.random.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n                             stddev=0.1,\n                             seed=SEED, dtype=data_type()))\n     conv1_biases = tf.Variable(tf.zeros([32], dtype=data_type()))\n-    conv2_weights = tf.Variable(tf.truncated_normal(\n+    conv2_weights = tf.Variable(tf.random.truncated_normal(\n         [5, 5, 32, 64], stddev=0.1,\n         seed=SEED, dtype=data_type()))\n     conv2_biases = tf.Variable(tf.constant(0.1, shape=[64], dtype=data_type()))\n     fc1_weights = tf.Variable(  # fully connected, depth 512.\n-        tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n+        tf.random.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n                             stddev=0.1,\n                             seed=SEED,\n                             dtype=data_type()))\n     fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=data_type()))\n     fc1p_weights = tf.Variable(  # fully connected, depth 512.\n-        tf.truncated_normal([512, 2],\n+        tf.random.truncated_normal([512, 2],\n                             stddev=0.1,\n                             seed=SEED,\n                             dtype=data_type()))\n     fc1p_biases = tf.Variable(tf.constant(0.1, shape=[2], dtype=data_type()))\n-    fc2_weights = tf.Variable(tf.truncated_normal([2, NUM_LABELS],\n+    fc2_weights = tf.Variable(tf.random.truncated_normal([2, NUM_LABELS],\n                                                   stddev=0.1,\n                                                   seed=SEED,\n                                                   dtype=data_type()))\n@@ -205,15 +205,15 @@ def main(argv=None):  # pylint: disable=unused-argument\n         Ref: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177\n         """\n         name = \'batch_norm\'\n-        with tf.variable_scope(name):\n-            phase_train = tf.convert_to_tensor(phase_train, dtype=tf.bool)\n+        with tf.compat.v1.variable_scope(name):\n+            phase_train = tf.convert_to_tensor(value=phase_train, dtype=tf.bool)\n             n_out = int(x.get_shape()[-1])\n             beta = tf.Variable(tf.constant(0.0, shape=[n_out], dtype=x.dtype),\n                                name=name+\'/beta\', trainable=True, dtype=x.dtype)\n             gamma = tf.Variable(tf.constant(1.0, shape=[n_out], dtype=x.dtype),\n                                 name=name+\'/gamma\', trainable=True, dtype=x.dtype)\n           \n-            batch_mean, batch_var = tf.nn.moments(x, [0], name=\'moments\')\n+            batch_mean, batch_var = tf.nn.moments(x=x, axes=[0], name=\'moments\')\n             ema = tf.train.ExponentialMovingAverage(decay=0.9)\n             def mean_var_with_update():\n                 ema_apply_op = ema.apply([batch_mean, batch_var])\n@@ -233,24 +233,24 @@ def main(argv=None):  # pylint: disable=unused-argument\n         # 2D convolution, with \'SAME\' padding (i.e. the output feature map has\n         # the same size as the input). Note that {strides} is a 4D array whose\n         # shape matches the data layout: [image index, y, x, depth].\n-        conv = tf.nn.conv2d(data,\n-                            conv1_weights,\n+        conv = tf.nn.conv2d(input=data,\n+                            filters=conv1_weights,\n                             strides=[1, 1, 1, 1],\n                             padding=\'SAME\')\n         # Bias and rectified linear non-linearity.\n         relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n         # Max pooling. The kernel size spec {ksize} also follows the layout of\n         # the data. Here we have a pooling window of 2, and a stride of 2.\n-        pool = tf.nn.max_pool(relu,\n+        pool = tf.nn.max_pool2d(input=relu,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\')\n-        conv = tf.nn.conv2d(pool,\n-                            conv2_weights,\n+        conv = tf.nn.conv2d(input=pool,\n+                            filters=conv2_weights,\n                             strides=[1, 1, 1, 1],\n                             padding=\'SAME\')\n         relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n-        pool = tf.nn.max_pool(relu,\n+        pool = tf.nn.max_pool2d(input=relu,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\')\n@@ -266,7 +266,7 @@ def main(argv=None):  # pylint: disable=unused-argument\n         # Add a 50% dropout during training only. Dropout also scales\n         # activations such that no rescaling is needed at evaluation time.\n         if train:\n-            hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n+            hidden = tf.nn.dropout(hidden, 1 - (0.5), seed=SEED)\n \n         hidden = tf.matmul(hidden, fc1p_weights) + fc1p_biases\n \n@@ -275,7 +275,7 @@ def main(argv=None):  # pylint: disable=unused-argument\n     # Training computation: logits + cross-entropy loss.\n     logits, hidden = model(train_data_node, True)\n     #logits = batch_norm(logits, True)\n-    xent_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n+    xent_loss = tf.reduce_mean(input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n         logits, train_labels_node))\n     beta = 1e-3\n     #center_loss, update_centers = center_loss_op(hidden, train_labels_node)\n@@ -292,14 +292,14 @@ def main(argv=None):  # pylint: disable=unused-argument\n     # controls the learning rate decay.\n     batch = tf.Variable(0, dtype=data_type())\n     # Decay once per epoch, using an exponential schedule starting at 0.01.\n-    learning_rate = tf.train.exponential_decay(\n+    learning_rate = tf.compat.v1.train.exponential_decay(\n         0.01,                # Base learning rate.\n         batch * BATCH_SIZE,  # Current index into the dataset.\n         train_size,          # Decay step.\n         0.95,                # Decay rate.\n         staircase=True)\n     # Use simple momentum for the optimization.\n-    optimizer = tf.train.MomentumOptimizer(learning_rate,\n+    optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate,\n                                            0.9).minimize(loss,\n                                                          global_step=batch)\n   \n@@ -353,9 +353,9 @@ def main(argv=None):  # pylint: disable=unused-argument\n \n     # Create a local session to run the training.\n     start_time = time.time()\n-    with tf.Session() as sess:\n+    with tf.compat.v1.Session() as sess:\n         # Run all the initializers to prepare the trainable parameters.\n-        tf.global_variables_initializer().run() #pylint: disable=no-member\n+        tf.compat.v1.global_variables_initializer().run() #pylint: disable=no-member\n         print(\'Initialized!\')\n         # Loop through training steps.\n         for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n@@ -401,4 +401,4 @@ def main(argv=None):  # pylint: disable=unused-argument\n \n \n if __name__ == \'__main__\':\n-    tf.app.run()\n+    tf.compat.v1.app.run()\ndiff --git a/tmp/mnist_noise_labels.py b/tmp/mnist_noise_labels.py\nindex d24e9a3..9d54ac7 100644\n--- a/tmp/mnist_noise_labels.py\n+++ b/tmp/mnist_noise_labels.py\n@@ -48,10 +48,10 @@ NOISE_FACTOR = 0.2\n BETA = 0.8\n \n \n-tf.app.flags.DEFINE_boolean("self_test", False, "True if running a self test.")\n-tf.app.flags.DEFINE_boolean(\'use_fp16\', False,\n+tf.compat.v1.app.flags.DEFINE_boolean("self_test", False, "True if running a self test.")\n+tf.compat.v1.app.flags.DEFINE_boolean(\'use_fp16\', False,\n                             "Use half floats instead of full floats if True.")\n-FLAGS = tf.app.flags.FLAGS\n+FLAGS = tf.compat.v1.app.flags.FLAGS\n \n \n def data_type():\n@@ -64,12 +64,12 @@ def data_type():\n \n def maybe_download(filename):\n     """Download the data from Yann\'s website, unless it\'s already here."""\n-    if not tf.gfile.Exists(WORK_DIRECTORY):\n-        tf.gfile.MakeDirs(WORK_DIRECTORY)\n+    if not tf.io.gfile.exists(WORK_DIRECTORY):\n+        tf.io.gfile.makedirs(WORK_DIRECTORY)\n     filepath = os.path.join(WORK_DIRECTORY, filename)\n-    if not tf.gfile.Exists(filepath):\n+    if not tf.io.gfile.exists(filepath):\n         filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n-        with tf.gfile.GFile(filepath) as f:\n+        with tf.io.gfile.GFile(filepath) as f:\n             size = f.size()\n         print(\'Successfully downloaded\', filename, size, \'bytes.\')\n     return filepath\n@@ -157,11 +157,11 @@ def main(argv=None):  # pylint: disable=unused-argument\n     # This is where training samples and labels are fed to the graph.\n     # These placeholder nodes will be fed a batch of training data at each\n     # training step using the {feed_dict} argument to the Run() call below.\n-    train_data_node = tf.placeholder(\n+    train_data_node = tf.compat.v1.placeholder(\n         data_type(),\n         shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n-    train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n-    eval_data = tf.placeholder(\n+    train_labels_node = tf.compat.v1.placeholder(tf.int64, shape=(BATCH_SIZE,))\n+    eval_data = tf.compat.v1.placeholder(\n         data_type(),\n         shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n \n@@ -169,21 +169,21 @@ def main(argv=None):  # pylint: disable=unused-argument\n     # initial value which will be assigned when we call:\n     # {tf.global_variables_initializer().run()}\n     conv1_weights = tf.Variable(\n-        tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n+        tf.random.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n                             stddev=0.1,\n                             seed=SEED, dtype=data_type()))\n     conv1_biases = tf.Variable(tf.zeros([32], dtype=data_type()))\n-    conv2_weights = tf.Variable(tf.truncated_normal(\n+    conv2_weights = tf.Variable(tf.random.truncated_normal(\n         [5, 5, 32, 64], stddev=0.1,\n         seed=SEED, dtype=data_type()))\n     conv2_biases = tf.Variable(tf.constant(0.1, shape=[64], dtype=data_type()))\n     fc1_weights = tf.Variable(  # fully connected, depth 512.\n-        tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n+        tf.random.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n                             stddev=0.1,\n                             seed=SEED,\n                             dtype=data_type()))\n     fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=data_type()))\n-    fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_LABELS],\n+    fc2_weights = tf.Variable(tf.random.truncated_normal([512, NUM_LABELS],\n                                                   stddev=0.1,\n                                                   seed=SEED,\n                                                   dtype=data_type()))\n@@ -197,24 +197,24 @@ def main(argv=None):  # pylint: disable=unused-argument\n         # 2D convolution, with \'SAME\' padding (i.e. the output feature map has\n         # the same size as the input). Note that {strides} is a 4D array whose\n         # shape matches the data layout: [image index, y, x, depth].\n-        conv = tf.nn.conv2d(data,\n-                            conv1_weights,\n+        conv = tf.nn.conv2d(input=data,\n+                            filters=conv1_weights,\n                             strides=[1, 1, 1, 1],\n                             padding=\'SAME\')\n         # Bias and rectified linear non-linearity.\n         relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n         # Max pooling. The kernel size spec {ksize} also follows the layout of\n         # the data. Here we have a pooling window of 2, and a stride of 2.\n-        pool = tf.nn.max_pool(relu,\n+        pool = tf.nn.max_pool2d(input=relu,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\')\n-        conv = tf.nn.conv2d(pool,\n-                            conv2_weights,\n+        conv = tf.nn.conv2d(input=pool,\n+                            filters=conv2_weights,\n                             strides=[1, 1, 1, 1],\n                             padding=\'SAME\')\n         relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n-        pool = tf.nn.max_pool(relu,\n+        pool = tf.nn.max_pool2d(input=relu,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\')\n@@ -231,7 +231,7 @@ def main(argv=None):  # pylint: disable=unused-argument\n         # Add a 50% dropout during training only. Dropout also scales\n         # activations such that no rescaling is needed at evaluation time.\n         if train:\n-            hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n+            hidden = tf.nn.dropout(hidden, 1 - (0.5), seed=SEED)\n         return tf.matmul(hidden, fc2_weights) + fc2_biases\n \n     # Training computation: logits + cross-entropy loss.\n@@ -243,12 +243,12 @@ def main(argv=None):  # pylint: disable=unused-argument\n \n     t = tf.one_hot(train_labels_node, NUM_LABELS)\n     q = tf.nn.softmax(logits)\n-    qqq = tf.arg_max(q, dimension=1)\n+    qqq = tf.argmax(q, axis=1)\n     z = tf.one_hot(qqq, NUM_LABELS)\n     #cross_entropy = -tf.reduce_sum(t*tf.log(q),reduction_indices=1)\n-    cross_entropy = -tf.reduce_sum((BETA*t+(1-BETA)*z)*tf.log(q),reduction_indices=1)\n+    cross_entropy = -tf.reduce_sum(input_tensor=(BETA*t+(1-BETA)*z)*tf.math.log(q),axis=1)\n     \n-    loss = tf.reduce_mean(cross_entropy)\n+    loss = tf.reduce_mean(input_tensor=cross_entropy)\n     \n #     loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n #         logits, train_labels_node))\n@@ -263,14 +263,14 @@ def main(argv=None):  # pylint: disable=unused-argument\n     # controls the learning rate decay.\n     batch = tf.Variable(0, dtype=data_type())\n     # Decay once per epoch, using an exponential schedule starting at 0.01.\n-    learning_rate = tf.train.exponential_decay(\n+    learning_rate = tf.compat.v1.train.exponential_decay(\n         0.01,                # Base learning rate.\n         batch * BATCH_SIZE,  # Current index into the dataset.\n         train_size,          # Decay step.\n         0.95,                # Decay rate.\n         staircase=True)\n     # Use simple momentum for the optimization.\n-    optimizer = tf.train.MomentumOptimizer(learning_rate,\n+    optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate,\n                                            0.9).minimize(loss,\n                                                          global_step=batch)\n   \n@@ -304,9 +304,9 @@ def main(argv=None):  # pylint: disable=unused-argument\n   \n     # Create a local session to run the training.\n     start_time = time.time()\n-    with tf.Session() as sess:\n+    with tf.compat.v1.Session() as sess:\n         # Run all the initializers to prepare the trainable parameters.\n-        tf.global_variables_initializer().run() #pylint: disable=no-member\n+        tf.compat.v1.global_variables_initializer().run() #pylint: disable=no-member\n         print(\'Initialized!\')\n         # Loop through training steps.\n         for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n@@ -344,4 +344,4 @@ def main(argv=None):  # pylint: disable=unused-argument\n \n \n if __name__ == \'__main__\':\n-    tf.app.run()\n+    tf.compat.v1.app.run()\ndiff --git a/tmp/mtcnn.py b/tmp/mtcnn.py\nindex 867fe0d..bd7f6e9 100644\n--- a/tmp/mtcnn.py\n+++ b/tmp/mtcnn.py\n@@ -30,18 +30,18 @@ from scipy import misc\n \n with tf.Graph().as_default():\n   \n-    sess = tf.Session()\n+    sess = tf.compat.v1.Session()\n     with sess.as_default():\n-        with tf.variable_scope(\'pnet\'):\n-            data = tf.placeholder(tf.float32, (None,None,None,3), \'input\')\n+        with tf.compat.v1.variable_scope(\'pnet\'):\n+            data = tf.compat.v1.placeholder(tf.float32, (None,None,None,3), \'input\')\n             pnet = align.detect_face.PNet({\'data\':data})\n             pnet.load(\'../../data/det1.npy\', sess)\n-        with tf.variable_scope(\'rnet\'):\n-            data = tf.placeholder(tf.float32, (None,24,24,3), \'input\')\n+        with tf.compat.v1.variable_scope(\'rnet\'):\n+            data = tf.compat.v1.placeholder(tf.float32, (None,24,24,3), \'input\')\n             rnet = align.detect_face.RNet({\'data\':data})\n             rnet.load(\'../../data/det2.npy\', sess)\n-        with tf.variable_scope(\'onet\'):\n-            data = tf.placeholder(tf.float32, (None,48,48,3), \'input\')\n+        with tf.compat.v1.variable_scope(\'onet\'):\n+            data = tf.compat.v1.placeholder(tf.float32, (None,48,48,3), \'input\')\n             onet = align.detect_face.ONet({\'data\':data})\n             onet.load(\'../../data/det3.npy\', sess)\n             \ndiff --git a/tmp/mtcnn_test.py b/tmp/mtcnn_test.py\nindex e02b11a..eba4302 100644\n--- a/tmp/mtcnn_test.py\n+++ b/tmp/mtcnn_test.py\n@@ -29,9 +29,9 @@ import align.detect_face\n \n g1 = tf.Graph()\n with g1.as_default():\n-    data = tf.placeholder(tf.float32, (None,None,None,3), \'input\')\n+    data = tf.compat.v1.placeholder(tf.float32, (None,None,None,3), \'input\')\n     pnet = align.detect_face.PNet({\'data\':data})\n-    sess1 = tf.Session(graph=g1)\n+    sess1 = tf.compat.v1.Session(graph=g1)\n     pnet.load(\'../../data/det1.npy\', sess1)\n     pnet_fun = lambda img : sess1.run((\'conv4-2/BiasAdd:0\', \'prob1:0\'), feed_dict={\'input:0\':img})\n np.random.seed(666)\n@@ -59,9 +59,9 @@ np.set_printoptions(formatter={\'float\': \'{: 0.4f}\'.format})\n \n g2 = tf.Graph()\n with g2.as_default():\n-    data = tf.placeholder(tf.float32, (None,24,24,3), \'input\')\n+    data = tf.compat.v1.placeholder(tf.float32, (None,24,24,3), \'input\')\n     rnet = align.detect_face.RNet({\'data\':data})\n-    sess2 = tf.Session(graph=g2)\n+    sess2 = tf.compat.v1.Session(graph=g2)\n     rnet.load(\'../../data/det2.npy\', sess2)\n     rnet_fun = lambda img : sess2.run((\'conv5-2/conv5-2:0\', \'prob1:0\'), feed_dict={\'input:0\':img})\n np.random.seed(666)\n@@ -85,9 +85,9 @@ img = np.transpose(img, (0,2,3,1))\n     \n g3 = tf.Graph()\n with g3.as_default():\n-    data = tf.placeholder(tf.float32, (None,48,48,3), \'input\')\n+    data = tf.compat.v1.placeholder(tf.float32, (None,48,48,3), \'input\')\n     onet = align.detect_face.ONet({\'data\':data})\n-    sess3 = tf.Session(graph=g3)\n+    sess3 = tf.compat.v1.Session(graph=g3)\n     onet.load(\'../../data/det3.npy\', sess3)\n     onet_fun = lambda img : sess3.run((\'conv6-2/conv6-2:0\', \'conv6-3/conv6-3:0\', \'prob1:0\'), feed_dict={\'input:0\':img})\n np.random.seed(666)\ndiff --git a/tmp/mtcnn_test_pnet_dbg.py b/tmp/mtcnn_test_pnet_dbg.py\nindex d4fdfbb..eb530a5 100644\n--- a/tmp/mtcnn_test_pnet_dbg.py\n+++ b/tmp/mtcnn_test_pnet_dbg.py\n@@ -9,11 +9,11 @@ import align.detect_face\n \n #ref = io.loadmat(\'pnet_dbg.mat\')\n with tf.Graph().as_default():\n-    sess = tf.Session()\n+    sess = tf.compat.v1.Session()\n     with sess.as_default():\n-        with tf.variable_scope(\'pnet\'):\n+        with tf.compat.v1.variable_scope(\'pnet\'):\n #            data = tf.placeholder(tf.float32, (None,None,None,3), \'input\')\n-            data = tf.placeholder(tf.float32, (1,1610, 1901,3), \'input\')\n+            data = tf.compat.v1.placeholder(tf.float32, (1,1610, 1901,3), \'input\')\n             pnet = align.detect_face.PNet({\'data\':data})\n             pnet.load(\'../../data/det1.npy\', sess)\n #         with tf.variable_scope(\'rnet\'):\ndiff --git a/tmp/network.py b/tmp/network.py\nindex c375e43..7999a4f 100644\n--- a/tmp/network.py\n+++ b/tmp/network.py\n@@ -33,30 +33,30 @@ from tensorflow.python.ops import control_flow_ops\n \n \n def conv(inpOp, nIn, nOut, kH, kW, dH, dW, padType, name, phase_train=True, use_batch_norm=True, weight_decay=0.0):\n-    with tf.variable_scope(name):\n+    with tf.compat.v1.variable_scope(name):\n         l2_regularizer = lambda t: l2_loss(t, weight=weight_decay)\n-        kernel = tf.get_variable("weights", [kH, kW, nIn, nOut],\n-            initializer=tf.truncated_normal_initializer(stddev=1e-1),\n+        kernel = tf.compat.v1.get_variable("weights", [kH, kW, nIn, nOut],\n+            initializer=tf.compat.v1.truncated_normal_initializer(stddev=1e-1),\n             regularizer=l2_regularizer, dtype=inpOp.dtype)\n-        cnv = tf.nn.conv2d(inpOp, kernel, [1, dH, dW, 1], padding=padType)\n+        cnv = tf.nn.conv2d(input=inpOp, filters=kernel, strides=[1, dH, dW, 1], padding=padType)\n         \n         if use_batch_norm:\n             conv_bn = batch_norm(cnv, phase_train)\n         else:\n             conv_bn = cnv\n-        biases = tf.get_variable("biases", [nOut], initializer=tf.constant_initializer(), dtype=inpOp.dtype)\n+        biases = tf.compat.v1.get_variable("biases", [nOut], initializer=tf.compat.v1.constant_initializer(), dtype=inpOp.dtype)\n         bias = tf.nn.bias_add(conv_bn, biases)\n         conv1 = tf.nn.relu(bias)\n     return conv1\n \n def affine(inpOp, nIn, nOut, name, weight_decay=0.0):\n-    with tf.variable_scope(name):\n+    with tf.compat.v1.variable_scope(name):\n         l2_regularizer = lambda t: l2_loss(t, weight=weight_decay)\n-        weights = tf.get_variable("weights", [nIn, nOut],\n-            initializer=tf.truncated_normal_initializer(stddev=1e-1),\n+        weights = tf.compat.v1.get_variable("weights", [nIn, nOut],\n+            initializer=tf.compat.v1.truncated_normal_initializer(stddev=1e-1),\n             regularizer=l2_regularizer, dtype=inpOp.dtype)\n-        biases = tf.get_variable("biases", [nOut], initializer=tf.constant_initializer(), dtype=inpOp.dtype)\n-        affine1 = tf.nn.relu_layer(inpOp, weights, biases)\n+        biases = tf.compat.v1.get_variable("biases", [nOut], initializer=tf.compat.v1.constant_initializer(), dtype=inpOp.dtype)\n+        affine1 = tf.compat.v1.nn.relu_layer(inpOp, weights, biases)\n     return affine1\n \n def l2_loss(tensor, weight=1.0, scope=None):\n@@ -68,21 +68,21 @@ def l2_loss(tensor, weight=1.0, scope=None):\n     Returns:\n       the L2 loss op.\n     """\n-    with tf.name_scope(scope):\n-        weight = tf.convert_to_tensor(weight,\n+    with tf.compat.v1.name_scope(scope):\n+        weight = tf.convert_to_tensor(value=weight,\n                                       dtype=tensor.dtype.base_dtype,\n                                       name=\'loss_weight\')\n         loss = tf.multiply(weight, tf.nn.l2_loss(tensor), name=\'value\')\n     return loss\n \n def lppool(inpOp, pnorm, kH, kW, dH, dW, padding, name):\n-    with tf.variable_scope(name):\n+    with tf.compat.v1.variable_scope(name):\n         if pnorm == 2:\n             pwr = tf.square(inpOp)\n         else:\n             pwr = tf.pow(inpOp, pnorm)\n           \n-        subsamp = tf.nn.avg_pool(pwr,\n+        subsamp = tf.nn.avg_pool2d(input=pwr,\n                               ksize=[1, kH, kW, 1],\n                               strides=[1, dH, dW, 1],\n                               padding=padding)\n@@ -96,16 +96,16 @@ def lppool(inpOp, pnorm, kH, kW, dH, dW, padding, name):\n     return out\n \n def mpool(inpOp, kH, kW, dH, dW, padding, name):\n-    with tf.variable_scope(name):\n-        maxpool = tf.nn.max_pool(inpOp,\n+    with tf.compat.v1.variable_scope(name):\n+        maxpool = tf.nn.max_pool2d(input=inpOp,\n                        ksize=[1, kH, kW, 1],\n                        strides=[1, dH, dW, 1],\n                        padding=padding)  \n     return maxpool\n \n def apool(inpOp, kH, kW, dH, dW, padding, name):\n-    with tf.variable_scope(name):\n-        avgpool = tf.nn.avg_pool(inpOp,\n+    with tf.compat.v1.variable_scope(name):\n+        avgpool = tf.nn.avg_pool2d(input=inpOp,\n                               ksize=[1, kH, kW, 1],\n                               strides=[1, dH, dW, 1],\n                               padding=padding)\n@@ -125,15 +125,15 @@ def batch_norm(x, phase_train):\n     Ref: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177\n     """\n     name = \'batch_norm\'\n-    with tf.variable_scope(name):\n-        phase_train = tf.convert_to_tensor(phase_train, dtype=tf.bool)\n+    with tf.compat.v1.variable_scope(name):\n+        phase_train = tf.convert_to_tensor(value=phase_train, dtype=tf.bool)\n         n_out = int(x.get_shape()[3])\n         beta = tf.Variable(tf.constant(0.0, shape=[n_out], dtype=x.dtype),\n                            name=name+\'/beta\', trainable=True, dtype=x.dtype)\n         gamma = tf.Variable(tf.constant(1.0, shape=[n_out], dtype=x.dtype),\n                             name=name+\'/gamma\', trainable=True, dtype=x.dtype)\n       \n-        batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name=\'moments\')\n+        batch_mean, batch_var = tf.nn.moments(x=x, axes=[0,1,2], name=\'moments\')\n         ema = tf.train.ExponentialMovingAverage(decay=0.9)\n         def mean_var_with_update():\n             ema_apply_op = ema.apply([batch_mean, batch_var])\n@@ -164,25 +164,25 @@ def inception(inp, inSize, ks, o1s, o2s1, o2s2, o3s1, o3s2, o4s1, o4s2, o4s3, po\n     \n     net = []\n     \n-    with tf.variable_scope(name):\n-        with tf.variable_scope(\'branch1_1x1\'):\n+    with tf.compat.v1.variable_scope(name):\n+        with tf.compat.v1.variable_scope(\'branch1_1x1\'):\n             if o1s>0:\n                 conv1 = conv(inp, inSize, o1s, 1, 1, 1, 1, \'SAME\', \'conv1x1\', phase_train=phase_train, use_batch_norm=use_batch_norm, weight_decay=weight_decay)\n                 net.append(conv1)\n       \n-        with tf.variable_scope(\'branch2_3x3\'):\n+        with tf.compat.v1.variable_scope(\'branch2_3x3\'):\n             if o2s1>0:\n                 conv3a = conv(inp, inSize, o2s1, 1, 1, 1, 1, \'SAME\', \'conv1x1\', phase_train=phase_train, use_batch_norm=use_batch_norm, weight_decay=weight_decay)\n                 conv3 = conv(conv3a, o2s1, o2s2, 3, 3, ks, ks, \'SAME\', \'conv3x3\', phase_train=phase_train, use_batch_norm=use_batch_norm, weight_decay=weight_decay)\n                 net.append(conv3)\n       \n-        with tf.variable_scope(\'branch3_5x5\'):\n+        with tf.compat.v1.variable_scope(\'branch3_5x5\'):\n             if o3s1>0:\n                 conv5a = conv(inp, inSize, o3s1, 1, 1, 1, 1, \'SAME\', \'conv1x1\', phase_train=phase_train, use_batch_norm=use_batch_norm, weight_decay=weight_decay)\n                 conv5 = conv(conv5a, o3s1, o3s2, 5, 5, ks, ks, \'SAME\', \'conv5x5\', phase_train=phase_train, use_batch_norm=use_batch_norm, weight_decay=weight_decay)\n                 net.append(conv5)\n       \n-        with tf.variable_scope(\'branch4_pool\'):\n+        with tf.compat.v1.variable_scope(\'branch4_pool\'):\n             if poolType==\'MAX\':\n                 pool = mpool(inp, o4s1, o4s1, o4s3, o4s3, \'SAME\', \'pool\')\n             elif poolType==\'L2\':\ndiff --git a/tmp/nn2.py b/tmp/nn2.py\nindex 7362653..5528a1a 100644\n--- a/tmp/nn2.py\n+++ b/tmp/nn2.py\n@@ -74,7 +74,7 @@ def inference(images, keep_probability, phase_train=True, weight_decay=0.0):\n     endpoints[\'pool6\'] = net\n     net = tf.reshape(net, [-1, 1024])\n     endpoints[\'prelogits\'] = net\n-    net = tf.nn.dropout(net, keep_probability)\n+    net = tf.nn.dropout(net, 1 - (keep_probability))\n     endpoints[\'dropout\'] = net\n     \n     return net, endpoints\ndiff --git a/tmp/nn3.py b/tmp/nn3.py\nindex 2e0502c..55da0d7 100644\n--- a/tmp/nn3.py\n+++ b/tmp/nn3.py\n@@ -74,7 +74,7 @@ def inference(images, keep_probability, phase_train=True, weight_decay=0.0):\n     endpoints[\'pool6\'] = net\n     net = tf.reshape(net, [-1, 1024])\n     endpoints[\'prelogits\'] = net\n-    net = tf.nn.dropout(net, keep_probability)\n+    net = tf.nn.dropout(net, 1 - (keep_probability))\n     endpoints[\'dropout\'] = net\n     \n     return net, endpoints\ndiff --git a/tmp/nn4.py b/tmp/nn4.py\nindex 8c3c79f..b161d58 100644\n--- a/tmp/nn4.py\n+++ b/tmp/nn4.py\n@@ -74,7 +74,7 @@ def inference(images, keep_probability, phase_train=True, weight_decay=0.0):\n     endpoints[\'pool6\'] = net\n     net = tf.reshape(net, [-1, 896])\n     endpoints[\'prelogits\'] = net\n-    net = tf.nn.dropout(net, keep_probability)\n+    net = tf.nn.dropout(net, 1 - (keep_probability))\n     endpoints[\'dropout\'] = net\n     \n     return net, endpoints\ndiff --git a/tmp/nn4_small2_v1.py b/tmp/nn4_small2_v1.py\nindex 780aafe..bf49804 100644\n--- a/tmp/nn4_small2_v1.py\n+++ b/tmp/nn4_small2_v1.py\n@@ -68,8 +68,7 @@ def inference(images, keep_probability, phase_train=True, weight_decay=0.0):\n     endpoints[\'pool6\'] = net\n     net = tf.reshape(net, [-1, 736])\n     endpoints[\'prelogits\'] = net\n-    net = tf.nn.dropout(net, keep_probability)\n+    net = tf.nn.dropout(net, 1 - (keep_probability))\n     endpoints[\'dropout\'] = net\n     \n     return net, endpoints\n-  \n\\ No newline at end of file\ndiff --git a/tmp/random_test.py b/tmp/random_test.py\nindex b186cc3..e95abdd 100644\n--- a/tmp/random_test.py\n+++ b/tmp/random_test.py\n@@ -4,11 +4,11 @@ from six.moves import xrange\n \n \n with tf.Graph().as_default():\n-  tf.set_random_seed(666)\n+  tf.compat.v1.set_random_seed(666)\n \n \n   # Placeholder for input images\n-  input_placeholder = tf.placeholder(tf.float32, shape=(9, 7), name=\'input\')\n+  input_placeholder = tf.compat.v1.placeholder(tf.float32, shape=(9, 7), name=\'input\')\n   \n   # Split example embeddings into anchor, positive and negative\n   #anchor, positive, negative = tf.split(0, 3, input)\n@@ -18,10 +18,10 @@ with tf.Graph().as_default():\n   negative = resh1[2,:,:]\n   \n   # Build an initialization operation to run below.\n-  init = tf.global_variables_initializer()\n+  init = tf.compat.v1.global_variables_initializer()\n \n   # Start running operations on the Graph.\n-  sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n+  sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=False))\n   sess.run(init)\n   \n   with sess.as_default():\ndiff --git a/tmp/seed_test.py b/tmp/seed_test.py\nindex 2077cf5..75ff662 100644\n--- a/tmp/seed_test.py\n+++ b/tmp/seed_test.py\n@@ -9,28 +9,28 @@ from tensorflow.python.ops import array_ops\n \n from six.moves import xrange\n \n-tf.app.flags.DEFINE_integer(\'batch_size\', 90,\n+tf.compat.v1.app.flags.DEFINE_integer(\'batch_size\', 90,\n                             """Number of images to process in a batch.""")\n-tf.app.flags.DEFINE_integer(\'image_size\', 96,\n+tf.compat.v1.app.flags.DEFINE_integer(\'image_size\', 96,\n                             """Image size (height, width) in pixels.""")\n-tf.app.flags.DEFINE_float(\'alpha\', 0.2,\n+tf.compat.v1.app.flags.DEFINE_float(\'alpha\', 0.2,\n                           """Positive to negative triplet distance margin.""")\n-tf.app.flags.DEFINE_float(\'learning_rate\', 0.1,\n+tf.compat.v1.app.flags.DEFINE_float(\'learning_rate\', 0.1,\n                           """Initial learning rate.""")\n-tf.app.flags.DEFINE_float(\'moving_average_decay\', 0.9999,\n+tf.compat.v1.app.flags.DEFINE_float(\'moving_average_decay\', 0.9999,\n                           """Expontential decay for tracking of training parameters.""")\n \n-FLAGS = tf.app.flags.FLAGS\n+FLAGS = tf.compat.v1.app.flags.FLAGS\n \n def run_train():\n   \n   with tf.Graph().as_default():\n   \n     # Set the seed for the graph\n-    tf.set_random_seed(666)\n+    tf.compat.v1.set_random_seed(666)\n \n     # Placeholder for input images\n-    images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, FLAGS.image_size, FLAGS.image_size, 3), name=\'input\')\n+    images_placeholder = tf.compat.v1.placeholder(tf.float32, shape=(FLAGS.batch_size, FLAGS.image_size, FLAGS.image_size, 3), name=\'input\')\n     \n     # Build the inference graph\n     embeddings = inference_conv_test(images_placeholder)\n@@ -47,22 +47,22 @@ def run_train():\n     #negative = resh1[2,:,:]\n     \n     # Calculate triplet loss\n-    pos_dist = tf.reduce_sum(tf.square(tf.sub(anchor, positive)), 1)\n-    neg_dist = tf.reduce_sum(tf.square(tf.sub(anchor, negative)), 1)\n+    pos_dist = tf.reduce_sum(input_tensor=tf.square(tf.sub(anchor, positive)), axis=1)\n+    neg_dist = tf.reduce_sum(input_tensor=tf.square(tf.sub(anchor, negative)), axis=1)\n     basic_loss = tf.add(tf.sub(pos_dist,neg_dist), FLAGS.alpha)\n-    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n+    loss = tf.reduce_mean(input_tensor=tf.maximum(basic_loss, 0.0), axis=0)\n \n     # Build a Graph that trains the model with one batch of examples and updates the model parameters\n-    opt = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n+    opt = tf.compat.v1.train.GradientDescentOptimizer(FLAGS.learning_rate)\n     #opt = tf.train.AdagradOptimizer(FLAGS.learning_rate)  # Optimizer does not seem to matter\n     grads = opt.compute_gradients(loss)\n     train_op = opt.apply_gradients(grads)\n     \n     # Initialize the variables\n-    init = tf.global_variables_initializer()\n+    init = tf.compat.v1.global_variables_initializer()\n     \n     # Launch the graph.\n-    sess = tf.Session()\n+    sess = tf.compat.v1.Session()\n     sess.run(init)\n \n     # Set the numpy seed\n@@ -76,7 +76,7 @@ def run_train():\n         batch = np.random.random((FLAGS.batch_size, FLAGS.image_size, FLAGS.image_size, 3))\n         feed_dict = { images_placeholder: batch }\n         # Get the variables\n-        var_names = tf.global_variables()\n+        var_names = tf.compat.v1.global_variables()\n         all_vars  += sess.run(var_names, feed_dict=feed_dict)\n         # Get the gradients\n         grad_tensors, grad_vars = zip(*grads)\n@@ -88,10 +88,10 @@ def run_train():\n   return (var_names, all_vars, grad_vars, grads_eval)\n \n def _conv(inpOp, nIn, nOut, kH, kW, dH, dW, padType):\n-  kernel = tf.Variable(tf.truncated_normal([kH, kW, nIn, nOut],\n+  kernel = tf.Variable(tf.random.truncated_normal([kH, kW, nIn, nOut],\n                                            dtype=tf.float32,\n                                            stddev=1e-1), name=\'weights\')\n-  conv = tf.nn.conv2d(inpOp, kernel, [1, dH, dW, 1], padding=padType)\n+  conv = tf.nn.conv2d(input=inpOp, filters=kernel, strides=[1, dH, dW, 1], padding=padType)\n   \n   biases = tf.Variable(tf.constant(0.0, shape=[nOut], dtype=tf.float32),\n                        trainable=True, name=\'biases\')\n@@ -100,12 +100,12 @@ def _conv(inpOp, nIn, nOut, kH, kW, dH, dW, padType):\n   return conv1\n \n def _affine(inpOp, nIn, nOut):\n-  kernel = tf.Variable(tf.truncated_normal([nIn, nOut],\n+  kernel = tf.Variable(tf.random.truncated_normal([nIn, nOut],\n                                            dtype=tf.float32,\n                                            stddev=1e-1), name=\'weights\')\n   biases = tf.Variable(tf.constant(0.0, shape=[nOut], dtype=tf.float32),\n                        trainable=True, name=\'biases\')\n-  affine1 = tf.nn.relu_layer(inpOp, kernel, biases)\n+  affine1 = tf.compat.v1.nn.relu_layer(inpOp, kernel, biases)\n   return affine1\n   \n def inference_conv_test(images):\ndiff --git a/tmp/select_triplets_test.py b/tmp/select_triplets_test.py\nindex 149e262..da88111 100644\n--- a/tmp/select_triplets_test.py\n+++ b/tmp/select_triplets_test.py\n@@ -2,11 +2,11 @@ import facenet\n import numpy as np\n import tensorflow as tf\n \n-FLAGS = tf.app.flags.FLAGS\n+FLAGS = tf.compat.v1.app.flags.FLAGS\n \n-tf.app.flags.DEFINE_integer(\'people_per_batch\', 45,\n+tf.compat.v1.app.flags.DEFINE_integer(\'people_per_batch\', 45,\n                             """Number of people per batch.""")\n-tf.app.flags.DEFINE_integer(\'alpha\', 0.2,\n+tf.compat.v1.app.flags.DEFINE_integer(\'alpha\', 0.2,\n                             """Positive to negative triplet distance margin.""")\n \n \ndiff --git a/tmp/test_invariance_on_lfw.py b/tmp/test_invariance_on_lfw.py\nindex 3bbbde0..bca431d 100644\n--- a/tmp/test_invariance_on_lfw.py\n+++ b/tmp/test_invariance_on_lfw.py\n@@ -47,16 +47,16 @@ def main(args):\n     \n     with tf.Graph().as_default():\n \n-        with tf.Session() as sess:\n+        with tf.compat.v1.Session() as sess:\n     \n             # Load the model\n             print(\'Loading model "%s"\' % args.model_file)\n             facenet.load_model(args.model_file)\n             \n             # Get input and output tensors\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n             image_size = int(images_placeholder.get_shape()[1])\n             \n             # Run test on LFW to check accuracy for different horizontal/vertical translations of input images\ndiff --git a/tmp/vggface16.py b/tmp/vggface16.py\nindex df45c53..6c45c8e 100644\n--- a/tmp/vggface16.py\n+++ b/tmp/vggface16.py\n@@ -25,47 +25,47 @@ def load(filename, images):\n     modelGraph = {}\n     modelGraph[\'input\'] = images\n     \n-    modelGraph[\'conv1_1\'] = tf.nn.conv2d(modelGraph[\'input\'], filter = vbbWeights(0), strides = [1, 1, 1, 1], padding = \'SAME\')\n+    modelGraph[\'conv1_1\'] = tf.nn.conv2d(input=modelGraph[\'input\'], filters = vbbWeights(0), strides = [1, 1, 1, 1], padding = \'SAME\')\n     modelGraph[\'relu1_1\'] = tf.nn.relu(modelGraph[\'conv1_1\'] + vbbConstants(0))\n-    modelGraph[\'conv1_2\'] = tf.nn.conv2d(modelGraph[\'relu1_1\'], filter = vbbWeights(2), strides = [1, 1, 1, 1], padding = \'SAME\')\n+    modelGraph[\'conv1_2\'] = tf.nn.conv2d(input=modelGraph[\'relu1_1\'], filters = vbbWeights(2), strides = [1, 1, 1, 1], padding = \'SAME\')\n     modelGraph[\'relu1_2\'] = tf.nn.relu(modelGraph[\'conv1_2\'] + vbbConstants(2))\n-    modelGraph[\'pool1\'] = tf.nn.max_pool(modelGraph[\'relu1_2\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n+    modelGraph[\'pool1\'] = tf.nn.max_pool2d(input=modelGraph[\'relu1_2\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n     \n-    modelGraph[\'conv2_1\'] = tf.nn.conv2d(modelGraph[\'pool1\'], filter = vbbWeights(5), strides = [1, 1, 1, 1], padding = \'SAME\')\n+    modelGraph[\'conv2_1\'] = tf.nn.conv2d(input=modelGraph[\'pool1\'], filters = vbbWeights(5), strides = [1, 1, 1, 1], padding = \'SAME\')\n     modelGraph[\'relu2_1\'] = tf.nn.relu(modelGraph[\'conv2_1\'] + vbbConstants(5))\n-    modelGraph[\'conv2_2\'] = tf.nn.conv2d(modelGraph[\'relu2_1\'], filter = vbbWeights(7), strides = [1, 1, 1, 1], padding = \'SAME\')\n+    modelGraph[\'conv2_2\'] = tf.nn.conv2d(input=modelGraph[\'relu2_1\'], filters = vbbWeights(7), strides = [1, 1, 1, 1], padding = \'SAME\')\n     modelGraph[\'relu2_2\'] = tf.nn.relu(modelGraph[\'conv2_2\'] + vbbConstants(7))\n-    modelGraph[\'pool2\'] = tf.nn.max_pool(modelGraph[\'relu2_2\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n+    modelGraph[\'pool2\'] = tf.nn.max_pool2d(input=modelGraph[\'relu2_2\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n     \n-    modelGraph[\'conv3_1\'] = tf.nn.conv2d(modelGraph[\'pool2\'], filter = vbbWeights(10), strides = [1, 1, 1, 1], padding = \'SAME\')\n+    modelGraph[\'conv3_1\'] = tf.nn.conv2d(input=modelGraph[\'pool2\'], filters = vbbWeights(10), strides = [1, 1, 1, 1], padding = \'SAME\')\n     modelGraph[\'relu3_1\'] = tf.nn.relu(modelGraph[\'conv3_1\'] + vbbConstants(10))\n-    modelGraph[\'conv3_2\'] = tf.nn.conv2d(modelGraph[\'relu3_1\'], filter = vbbWeights(12), strides = [1, 1, 1, 1], padding = \'SAME\')\n+    modelGraph[\'conv3_2\'] = tf.nn.conv2d(input=modelGraph[\'relu3_1\'], filters = vbbWeights(12), strides = [1, 1, 1, 1], padding = \'SAME\')\n     modelGraph[\'relu3_2\'] = tf.nn.relu(modelGraph[\'conv3_2\'] + vbbConstants(12))\n-    modelGraph[\'conv3_3\'] = tf.nn.conv2d(modelGraph[\'relu3_2\'], filter = vbbWeights(14), strides = [1, 1, 1, 1], padding = \'SAME\')\n+    modelGraph[\'conv3_3\'] = tf.nn.conv2d(input=modelGraph[\'relu3_2\'], filters = vbbWeights(14), strides = [1, 1, 1, 1], padding = \'SAME\')\n     modelGraph[\'relu3_3\'] = tf.nn.relu(modelGraph[\'conv3_3\'] + vbbConstants(14))\n-    modelGraph[\'pool3\'] = tf.nn.max_pool(modelGraph[\'relu3_3\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n+    modelGraph[\'pool3\'] = tf.nn.max_pool2d(input=modelGraph[\'relu3_3\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n     \n-    modelGraph[\'conv4_1\'] = tf.nn.conv2d(modelGraph[\'pool3\'], filter = vbbWeights(17), strides = [1, 1, 1, 1], padding = \'SAME\')\n+    modelGraph[\'conv4_1\'] = tf.nn.conv2d(input=modelGraph[\'pool3\'], filters = vbbWeights(17), strides = [1, 1, 1, 1], padding = \'SAME\')\n     modelGraph[\'relu4_1\'] = tf.nn.relu(modelGraph[\'conv4_1\'] + vbbConstants(17))\n-    modelGraph[\'conv4_2\'] = tf.nn.conv2d(modelGraph[\'relu4_1\'], filter = vbbWeights(19), strides = [1, 1, 1, 1], padding = \'SAME\')\n+    modelGraph[\'conv4_2\'] = tf.nn.conv2d(input=modelGraph[\'relu4_1\'], filters = vbbWeights(19), strides = [1, 1, 1, 1], padding = \'SAME\')\n     modelGraph[\'relu4_2\'] = tf.nn.relu(modelGraph[\'conv4_2\'] + vbbConstants(19))\n-    modelGraph[\'conv4_3\'] = tf.nn.conv2d(modelGraph[\'relu4_2\'], filter = vbbWeights(21), strides = [1, 1, 1, 1], padding = \'SAME\')\n+    modelGraph[\'conv4_3\'] = tf.nn.conv2d(input=modelGraph[\'relu4_2\'], filters = vbbWeights(21), strides = [1, 1, 1, 1], padding = \'SAME\')\n     modelGraph[\'relu4_3\'] = tf.nn.relu(modelGraph[\'conv4_3\'] + vbbConstants(21))\n-    modelGraph[\'pool4\'] = tf.nn.max_pool(modelGraph[\'relu4_3\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n+    modelGraph[\'pool4\'] = tf.nn.max_pool2d(input=modelGraph[\'relu4_3\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n     \n-    modelGraph[\'conv5_1\'] = tf.nn.conv2d(modelGraph[\'pool4\'], filter = vbbWeights(24), strides = [1, 1, 1, 1], padding = \'SAME\')\n+    modelGraph[\'conv5_1\'] = tf.nn.conv2d(input=modelGraph[\'pool4\'], filters = vbbWeights(24), strides = [1, 1, 1, 1], padding = \'SAME\')\n     modelGraph[\'relu5_1\'] = tf.nn.relu(modelGraph[\'conv5_1\'] + vbbConstants(24))\n-    modelGraph[\'conv5_2\'] = tf.nn.conv2d(modelGraph[\'relu5_1\'], filter = vbbWeights(26), strides = [1, 1, 1, 1], padding = \'SAME\')\n+    modelGraph[\'conv5_2\'] = tf.nn.conv2d(input=modelGraph[\'relu5_1\'], filters = vbbWeights(26), strides = [1, 1, 1, 1], padding = \'SAME\')\n     modelGraph[\'relu5_2\'] = tf.nn.relu(modelGraph[\'conv5_2\'] + vbbConstants(26))\n-    modelGraph[\'conv5_3\'] = tf.nn.conv2d(modelGraph[\'relu5_2\'], filter = vbbWeights(28), strides = [1, 1, 1, 1], padding = \'SAME\')\n+    modelGraph[\'conv5_3\'] = tf.nn.conv2d(input=modelGraph[\'relu5_2\'], filters = vbbWeights(28), strides = [1, 1, 1, 1], padding = \'SAME\')\n     modelGraph[\'relu5_3\'] = tf.nn.relu(modelGraph[\'conv5_3\'] + vbbConstants(28))\n-    modelGraph[\'pool5\'] = tf.nn.max_pool(modelGraph[\'relu5_3\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n+    modelGraph[\'pool5\'] = tf.nn.max_pool2d(input=modelGraph[\'relu5_3\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n     \n     modelGraph[\'resh1\'] = tf.reshape(modelGraph[\'pool5\'], [-1, 25088])\n-    modelGraph[\'fc6\'] = tf.nn.relu_layer(modelGraph[\'resh1\'], tf.reshape(vbbWeights(31), [25088, 4096]), vbbConstants(31))\n-    modelGraph[\'dropout1\'] = tf.nn.dropout(modelGraph[\'fc6\'], 0.5)\n-    modelGraph[\'fc7\'] = tf.nn.relu_layer(modelGraph[\'dropout1\'], tf.squeeze(vbbWeights(34), [0, 1]), vbbConstants(34))\n-    modelGraph[\'dropout2\'] = tf.nn.dropout(modelGraph[\'fc7\'], 0.5)\n-    modelGraph[\'fc8\'] = tf.nn.relu_layer(modelGraph[\'dropout2\'], tf.squeeze(vbbWeights(37), [0, 1]), vbbConstants(37))\n+    modelGraph[\'fc6\'] = tf.compat.v1.nn.relu_layer(modelGraph[\'resh1\'], tf.reshape(vbbWeights(31), [25088, 4096]), vbbConstants(31))\n+    modelGraph[\'dropout1\'] = tf.nn.dropout(modelGraph[\'fc6\'], 1 - (0.5))\n+    modelGraph[\'fc7\'] = tf.compat.v1.nn.relu_layer(modelGraph[\'dropout1\'], tf.squeeze(vbbWeights(34), [0, 1]), vbbConstants(34))\n+    modelGraph[\'dropout2\'] = tf.nn.dropout(modelGraph[\'fc7\'], 1 - (0.5))\n+    modelGraph[\'fc8\'] = tf.compat.v1.nn.relu_layer(modelGraph[\'dropout2\'], tf.squeeze(vbbWeights(37), [0, 1]), vbbConstants(37))\n \n     return modelGraph\ndiff --git a/tmp/vggverydeep19.py b/tmp/vggverydeep19.py\nindex 86f22c5..3366a5e 100644\n--- a/tmp/vggverydeep19.py\n+++ b/tmp/vggverydeep19.py\n@@ -23,27 +23,27 @@ def load(filename, images):\n     \n     modelGraph = {}\n     modelGraph[\'input\'] = images\n-    modelGraph[\'conv1_1\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'input\'], filter = vbbWeights(0), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(0))\n-    modelGraph[\'conv1_2\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv1_1\'], filter = vbbWeights(2), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(2))\n-    modelGraph[\'avgpool1\'] = tf.nn.avg_pool(modelGraph[\'conv1_2\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n-    modelGraph[\'conv2_1\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'avgpool1\'], filter = vbbWeights(5), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(5))\n-    modelGraph[\'conv2_2\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv2_1\'], filter = vbbWeights(7), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(7))\n-    modelGraph[\'avgpool2\'] = tf.nn.avg_pool(modelGraph[\'conv2_2\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n-    modelGraph[\'conv3_1\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'avgpool2\'], filter = vbbWeights(10), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(10))\n-    modelGraph[\'conv3_2\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv3_1\'], filter = vbbWeights(12), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(12))\n-    modelGraph[\'conv3_3\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv3_2\'], filter = vbbWeights(14), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(14))\n-    modelGraph[\'conv3_4\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv3_3\'], filter = vbbWeights(16), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(16))\n-    modelGraph[\'avgpool3\'] = tf.nn.avg_pool(modelGraph[\'conv3_4\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n-    modelGraph[\'conv4_1\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'avgpool3\'], filter = vbbWeights(19), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(19))\n-    modelGraph[\'conv4_2\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv4_1\'], filter = vbbWeights(21), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(21))\n-    modelGraph[\'conv4_3\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv4_2\'], filter = vbbWeights(23), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(23))\n-    modelGraph[\'conv4_4\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv4_3\'], filter = vbbWeights(25), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(25))\n-    modelGraph[\'avgpool4\'] = tf.nn.avg_pool(modelGraph[\'conv4_4\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n-    modelGraph[\'conv5_1\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'avgpool4\'], filter = vbbWeights(28), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(28))\n-    modelGraph[\'conv5_2\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv5_1\'], filter = vbbWeights(30), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(30))\n-    modelGraph[\'conv5_3\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv5_2\'], filter = vbbWeights(32), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(32))\n-    modelGraph[\'conv5_4\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv5_3\'], filter = vbbWeights(34), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(34))\n-    modelGraph[\'avgpool5\'] = tf.nn.avg_pool(modelGraph[\'conv5_4\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n+    modelGraph[\'conv1_1\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'input\'], filters = vbbWeights(0), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(0))\n+    modelGraph[\'conv1_2\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'conv1_1\'], filters = vbbWeights(2), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(2))\n+    modelGraph[\'avgpool1\'] = tf.nn.avg_pool2d(input=modelGraph[\'conv1_2\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n+    modelGraph[\'conv2_1\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'avgpool1\'], filters = vbbWeights(5), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(5))\n+    modelGraph[\'conv2_2\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'conv2_1\'], filters = vbbWeights(7), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(7))\n+    modelGraph[\'avgpool2\'] = tf.nn.avg_pool2d(input=modelGraph[\'conv2_2\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n+    modelGraph[\'conv3_1\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'avgpool2\'], filters = vbbWeights(10), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(10))\n+    modelGraph[\'conv3_2\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'conv3_1\'], filters = vbbWeights(12), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(12))\n+    modelGraph[\'conv3_3\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'conv3_2\'], filters = vbbWeights(14), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(14))\n+    modelGraph[\'conv3_4\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'conv3_3\'], filters = vbbWeights(16), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(16))\n+    modelGraph[\'avgpool3\'] = tf.nn.avg_pool2d(input=modelGraph[\'conv3_4\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n+    modelGraph[\'conv4_1\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'avgpool3\'], filters = vbbWeights(19), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(19))\n+    modelGraph[\'conv4_2\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'conv4_1\'], filters = vbbWeights(21), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(21))\n+    modelGraph[\'conv4_3\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'conv4_2\'], filters = vbbWeights(23), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(23))\n+    modelGraph[\'conv4_4\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'conv4_3\'], filters = vbbWeights(25), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(25))\n+    modelGraph[\'avgpool4\'] = tf.nn.avg_pool2d(input=modelGraph[\'conv4_4\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n+    modelGraph[\'conv5_1\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'avgpool4\'], filters = vbbWeights(28), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(28))\n+    modelGraph[\'conv5_2\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'conv5_1\'], filters = vbbWeights(30), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(30))\n+    modelGraph[\'conv5_3\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'conv5_2\'], filters = vbbWeights(32), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(32))\n+    modelGraph[\'conv5_4\'] = tf.nn.relu(tf.nn.conv2d(input=modelGraph[\'conv5_3\'], filters = vbbWeights(34), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(34))\n+    modelGraph[\'avgpool5\'] = tf.nn.avg_pool2d(input=modelGraph[\'conv5_4\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n     \n     return modelGraph\n \ndiff --git a/tmp/visualize.py b/tmp/visualize.py\nindex 6e5ea68..7679390 100644\n--- a/tmp/visualize.py\n+++ b/tmp/visualize.py\n@@ -43,9 +43,9 @@ def main(args):\n     np.random.seed(seed=args.seed)\n     img_noise = np.random.uniform(size=(args.image_size,args.image_size,3)) + 100.0\n   \n-    sess = tf.Session()\n+    sess = tf.compat.v1.Session()\n   \n-    t_input = tf.placeholder(np.float32, shape=(args.image_size,args.image_size,3), name=\'input\') # define the input tensor\n+    t_input = tf.compat.v1.placeholder(np.float32, shape=(args.image_size,args.image_size,3), name=\'input\') # define the input tensor\n     image_mean = 117.0\n     t_preprocessed = tf.expand_dims(t_input-image_mean, 0)\n      \n@@ -54,12 +54,12 @@ def main(args):\n             phase_train=True, weight_decay=0.0)\n       \n     # Create a saver for restoring variables\n-    saver = tf.train.Saver(tf.global_variables())\n+    saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables())\n   \n     # Restore the parameters\n     saver.restore(sess, args.model_file)\n   \n-    layers = [op.name for op in tf.get_default_graph().get_operations() if op.type==\'Conv2D\']\n+    layers = [op.name for op in tf.compat.v1.get_default_graph().get_operations() if op.type==\'Conv2D\']\n     feature_nums = {layer: int(T(layer).get_shape()[-1]) for layer in layers}\n   \n     print(\'Number of layers: %d\' % len(layers))\n@@ -85,15 +85,15 @@ def main(args):\n \n def T(layer):\n     \'\'\'Helper for getting layer output tensor\'\'\'\n-    return tf.get_default_graph().get_tensor_by_name(\'%s:0\' % layer)\n+    return tf.compat.v1.get_default_graph().get_tensor_by_name(\'%s:0\' % layer)\n \n def visstd(a, s=0.1):\n     \'\'\'Normalize the image range for visualization\'\'\'\n     return (a-a.mean())/max(a.std(), 1e-4)*s + 0.5\n \n def render_naive(sess, t_input, t_obj, img0, iter_n=20, step=1.0):\n-    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n-    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n+    t_score = tf.reduce_mean(input_tensor=t_obj) # defining the optimization objective\n+    t_grad = tf.gradients(ys=t_score, xs=t_input)[0] # behold the power of automatic differentiation!\n     \n     img = img0.copy()\n     for _ in range(iter_n):\ndiff --git a/tmp/visualize_vgg_model.py b/tmp/visualize_vgg_model.py\nindex 9468936..23cd60e 100644\n--- a/tmp/visualize_vgg_model.py\n+++ b/tmp/visualize_vgg_model.py\n@@ -33,7 +33,7 @@ def sqErrorLossContent(sess, modelGraph, layer):\n     #pylint: disable=maybe-no-member\n     N = p.shape[3]\n     M = p.shape[1] * p.shape[2]\n-    return (1 / (4 * N * M)) * tf.reduce_sum(tf.pow(modelGraph[layer] - sess.run(modelGraph[layer]), 2))\n+    return (1 / (4 * N * M)) * tf.reduce_sum(input_tensor=tf.pow(modelGraph[layer] - sess.run(modelGraph[layer]), 2))\n  \n # Squared-error loss of style between the two feature representations\n styleLayers = [\n@@ -47,14 +47,14 @@ def sqErrorLossStyle(sess, modelGraph):\n     def intermediateCalc(x, y):\n         N = x.shape[3]\n         M = x.shape[1] * x.shape[2]\n-        A = tf.matmul(tf.transpose(tf.reshape(x, (M, N))), tf.reshape(x, (M, N)))\n-        G = tf.matmul(tf.transpose(tf.reshape(y, (M, N))), tf.reshape(y, (M, N)))\n-        return (1 / (4 * N**2 * M**2)) * tf.reduce_sum(tf.pow(G - A, 2))\n+        A = tf.matmul(tf.transpose(a=tf.reshape(x, (M, N))), tf.reshape(x, (M, N)))\n+        G = tf.matmul(tf.transpose(a=tf.reshape(y, (M, N))), tf.reshape(y, (M, N)))\n+        return (1 / (4 * N**2 * M**2)) * tf.reduce_sum(input_tensor=tf.pow(G - A, 2))\n     E = [intermediateCalc(sess.run(modelGraph[layerName]), modelGraph[layerName]) for layerName, _ in styleLayers]\n     W = [w for _, w in styleLayers]\n     return sum([W[layerNumber] * E[layerNumber] for layerNumber in range(len(styleLayers))])\n \n-session = tf.InteractiveSession()\n+session = tf.compat.v1.InteractiveSession()\n  \n # Addition of extra dimension to image\n inputImage = np.reshape(inputImage, ((1,) + inputImage.shape))\n@@ -74,20 +74,20 @@ mixedImage = imageNoise * noiseRatio + inputImage * (1 - noiseRatio)\n pyplot.imshow(inputImage[0])\n \n \n-session.run(tf.global_variables_initializer())\n+session.run(tf.compat.v1.global_variables_initializer())\n session.run(nodes[\'input\'].assign(inputImage))\n contentLoss = sqErrorLossContent(session, nodes, \'conv4_2\')\n session.run(nodes[\'input\'].assign(paintingStyleImage))\n styleLoss = sqErrorLossStyle(session, nodes)\n totalLoss = beta * contentLoss + alpha * styleLoss\n \n-optimizer = tf.train.AdamOptimizer(2.0)\n+optimizer = tf.compat.v1.train.AdamOptimizer(2.0)\n trainStep = optimizer.minimize(totalLoss)\n-session.run(tf.global_variables_initializer())\n+session.run(tf.compat.v1.global_variables_initializer())\n session.run(nodes[\'input\'].assign(inputImage))\n # Number of iterations to run.\n iterations = 2000\n-session.run(tf.global_variables_initializer())\n+session.run(tf.compat.v1.global_variables_initializer())\n session.run(nodes[\'input\'].assign(inputImage))\n  \n for iters in range(iterations):\ndiff --git a/tmp/visualize_vggface.py b/tmp/visualize_vggface.py\nindex c34004c..853f683 100644\n--- a/tmp/visualize_vggface.py\n+++ b/tmp/visualize_vggface.py\n@@ -5,9 +5,9 @@ import tmp.vggface16\n \n def main():\n   \n-    sess = tf.Session()\n+    sess = tf.compat.v1.Session()\n   \n-    t_input = tf.placeholder(np.float32, name=\'input\') # define the input tensor\n+    t_input = tf.compat.v1.placeholder(np.float32, name=\'input\') # define the input tensor\n     image_mean = 117.0\n     t_preprocessed = tf.expand_dims(t_input-image_mean, 0)\n      \n@@ -33,8 +33,8 @@ def visstd(a, s=0.1):\n     return (a-a.mean())/max(a.std(), 1e-4)*s + 0.5\n \n def render_naive(sess, t_input, t_obj, img0, iter_n=20, step=1.0):\n-    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n-    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n+    t_score = tf.reduce_mean(input_tensor=t_obj) # defining the optimization objective\n+    t_grad = tf.gradients(ys=t_score, xs=t_input)[0] # behold the power of automatic differentiation!\n     \n     img = img0.copy()\n     for _ in range(iter_n):'